{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, PreTrainedTokenizer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\[*\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    #removes unicodes left in text so model does not learn unicodes\n",
    "    text = re.sub('^\\\\\\\\u[\\d\\D]{4}|-|σ|→|\\\\\\\\xad', '', text)\n",
    "    # Remove a sentence if it is only one word long\n",
    "    if len(text) > 2:\n",
    "        return ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokens = tokenizer.tokenize(str(text), add_special_tokens=True)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatizer(text):\n",
    "    if text == None:\n",
    "        print(\"Excuse me but there is an unexpected None value from cleaning the text!\")\n",
    "        return \"\"\n",
    "    sent = []\n",
    "    tokens = []\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feed2vec(filepath, tokenize=None):\n",
    "    global sentences, tokens\n",
    "    #Creates a pandas dataframe for the text data from a json file\n",
    "    df = pd.read_json(filepath)\n",
    "    \n",
    "    #Adds the column label text\n",
    "    df.columns = ['text']\n",
    "    \n",
    "    #Text clean function is applied to the pandas dataframe\n",
    "    df_clean = pd.DataFrame(df.text.apply(lambda x: clean_text(x)))\n",
    "    \n",
    "    #filters out all the None values in the cleaned dataset\n",
    "    #loc combines the operations in brackets into one single operation to avoid chaining indexes operations together\n",
    "    #copy explicitly tells pandas to make copy when creating master_of_none\n",
    "    #this is so later on only the copy is modified and there is no confusion between the copy and the original\n",
    "    master_of_none = df_clean.loc[df_clean.text.notnull()].copy()\n",
    "    \n",
    "    if tokenize == None:\n",
    "        tokens = None\n",
    "        #Lemmatizer function is applied to cleaned text with the none values removed\n",
    "        master_of_none[\"text_lemmatized\"] =  master_of_none.apply(lambda x: lemmatizer(x['text']), axis=1)\n",
    "        master_of_none['text_lemmatize_clean'] = master_of_none['text_lemmatized'].str.replace('-PRON-', '')\n",
    "        vocab = master_of_none['text_lemmatize_clean']\n",
    "        \n",
    "        sentences = [row.split() for row in vocab]\n",
    "        word_freq = defaultdict(int)\n",
    "        for sent in sentences:\n",
    "            for i in sent:\n",
    "                word_freq[i] += 1\n",
    "        len(word_freq)\n",
    "        \n",
    "    else:\n",
    "        tokens = []\n",
    "        sentences = sent_tokenize(str(master_of_none[\"text\"]))\n",
    "        word_freq = defaultdict(int)\n",
    "        for sent in sentences:\n",
    "            tokens.append(tokenizer(sent))\n",
    "            for i in sent:\n",
    "                word_freq[i] += 1\n",
    "        len(word_freq)\n",
    "    \n",
    "\n",
    "    return print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_train(w2vmodel, last_model = False):\n",
    "    global sentences, tokens\n",
    "    if tokens is not None:\n",
    "        words = tokens\n",
    "    else:\n",
    "        words = sentences\n",
    "    \n",
    "    if last_model == True:\n",
    "        #Previously updated model is loaded\n",
    "        w2v_model = Word2Vec.load(last_model)  \n",
    "        \n",
    "        #model vocabulary is updated\n",
    "        w2v_model.build_vocab(words, update=True)\n",
    "        \n",
    "    elif last_model == False:\n",
    "        # min_count: minimum number of occurrences of a word in the corpus to be included in the model.\n",
    "        # window: the maximum distance between the current and predicted word within a sentence.\n",
    "        # size: the dimensionality of the feature vectors\n",
    "        # workers: the number of cores your computer has\n",
    "        w2v_model = Word2Vec(min_count=20,\n",
    "                             window=5,\n",
    "                             size=400,\n",
    "                             workers=2)\n",
    "        #the new model's vocabulary is built \n",
    "        w2v_model.build_vocab(words)\n",
    "        \n",
    "    # train word vectors\n",
    "    #returns the number of words in the vocab and the number of words in the corpus\n",
    "    try:\n",
    "        w2v_model.train(words, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)\n",
    "    except RuntimeError:\n",
    "        print(\"Vocab was not built. Check your w2v parameters and try again!\")\n",
    "    #either the new or updated version of the w2v model is saved\n",
    "    w2v_model.save(w2vmodel)\n",
    "    return print(\"Training complete!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_word = ['alcohol', 'ketone', 'alkene', 'carbon', 'proton', 'polymer', 'acid', 'oxidize', 'anion', 'electrophile', 'polar', 'positive', 'mechanism', 'resonance', 'synthesis', 'isomer', 'heat', 'aromatic', ]\n",
    "second_word = ['hydroxyl', 'carbonyl', 'alkyne', 'nitrogen', 'hydrogen', 'chain', 'base', 'reduce', 'cation', 'nucleophile', 'nonpolar', 'negative', 'atom', 'solvent', 'electron', 'reaction', 'bond', 'equilibrium']\n",
    "my_headers = ['model_name', 'alcohol + hydroxyl', 'ketone + carbonyl', 'alkene + alkyyne', 'carbon + nitrogen', 'proton + hydrogen', 'polymer + chain', 'acid + base', 'oxidize + reduce', 'anion + cation', 'electrophile + nucleophile', 'polar + nonpolar', 'positive + negative', 'mechanism + atom', 'resonance + solvent', 'synthesis + electron', 'isomer + reaction', 'heat + bond', 'aromatic + equilibrium', 'Top 10 Carbon']\n",
    "\n",
    "def cosine_sim(w2vmodel, first_word, second_word):\n",
    "    global w2v_data\n",
    "    model_name = w2vmodel\n",
    "    \n",
    "    w2v_model = Word2Vec.load(w2vmodel)\n",
    "    \n",
    "    w2v_data = []\n",
    "    \n",
    "    w2v_data.append(model_name)\n",
    "    \n",
    "    for word1, word2 in zip(first_word, second_word):\n",
    "        try:\n",
    "        #synonym, antonym, or neutral pairs\n",
    "            cos_sim = w2v_model.wv.similarity(word1, word2)\n",
    "            w2v_data.append(cos_sim)\n",
    "        except KeyError:\n",
    "            cos_sim = 0 \n",
    "            w2v_data.append(cos_sim)\n",
    "            print(f\"{word1} or {word2} was not in the vocabulary\")\n",
    "   \n",
    "    try:\n",
    "        top10 = w2v_model.wv.most_similar(positive=['carbon'])\n",
    "        w2v_data.append(top10)\n",
    "    except KeyError:\n",
    "        top10 = 0\n",
    "        print(\"carbon is not in the vocabulary\")\n",
    "    \n",
    "    \n",
    "    return print(\"Data collection and saving complete!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_saver(excel_file, my_headers, new_file = None):\n",
    "    try:\n",
    "        if new_file is not None:\n",
    "            with open(excel_file, 'w',  newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow(my_headers)\n",
    "                csvwriter.writerow(w2v_data)\n",
    "        else:\n",
    "            with open(excel_file, 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow(w2v_data)\n",
    "    except PermissionError:\n",
    "        print(\"Excel file is most likely open. Close it before running program\")\n",
    "    return print(\"Data saved!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_grapher(w2vmodel):\n",
    "    \"Create TSNE model and plot it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    model = Word2Vec.load(w2vmodel)\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(18, 18)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'ChemLibre_JSONS/Basic_Principles_of_Organic_Chemistry_Roberts_and_Caserio.json'\n",
    "file2 = 'ChemLibre_JSONS/Bruice_Map.json'\n",
    "file3 = 'ChemLibre_JSONS/Catalytic_Asymmetric_Synthesis_Punniyamurthy.json'\n",
    "file4 = 'ChemLibre_JSONS/Environmental_Chemistry.json'\n",
    "file5 = 'ChemLibre_JSONS/How_to_be_a_Successful_Organic_Chemist_Sandtorv.json'\n",
    "file6 = 'ChemLibre_JSONS/Logic_of_Organic_Synthesis_Rao.json'\n",
    "file7 = 'ChemLibre_JSONS/Organic_Chemistry_A _Carbonyl_Early_Approach_McMichael.json'\n",
    "file8 = 'ChemLibre_JSONS/Organic_Chemistry_Lab_Techniques_Nichols.json'\n",
    "file9 = 'ChemLibre_JSONS/Organic_Chemistry_with_a_Biological_Emphasis_Soderberg.json'\n",
    "file10 = 'ChemLibre_JSONS/Polymer_Chemistry.json'\n",
    "file11 = 'ChemLibre_JSONS/Radical_Reactions_of_Carbohydrates_Binkley.json'\n",
    "file12 = 'ChemLibre_JSONS/Schaller_Polymer.json'\n",
    "file13 = 'ChemLibre_JSONS/Supplemental_Modules.json'\n",
    "file14 = 'ChemLibre_JSONS/Wade_Map.json'\n",
    "file15 = 'Springer_PDF/Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json'\n",
    "file16 = 'Springer_PDF/Advanced_Organic_Chemistry_Part_A_Structure_and_Mechanisms_by_Carey_and_Sundberg.json'\n",
    "file17 = 'Springer_PDF/Advanced_Organic_Chemistry_Part_B_Reactions_and_Synthesis_by_Carey_and_Sundberg.json'\n",
    "file18 = 'Springer_PDF/Principles_of_Polymer_Chemistry_by_Ravve.json'\n",
    "file19 = 'Springer_PDF/Polymer_Synthesis_Theory_and_Practice_by_Braun_Cherdron_Rehahn_Ritter_and_Voit.json'\n",
    "file20 = 'Springer_PDF/Polymer_Chemistry_by_Koltzsenburg_Maskos_and_Nuyken.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = 'robers_and_caserio.model'\n",
    "model2 = 'bruice.model'\n",
    "model3 = 'punniyamurthy.model'\n",
    "model4 = 'environmental.model'\n",
    "model5 = 'sandtorv.model'\n",
    "model6 = 'rao.model'\n",
    "model7 = 'mcmichael.model'\n",
    "model8 = 'nichols.model'\n",
    "model9 = 'soderberg.model'\n",
    "model10 = 'polymer.model'\n",
    "model11 = 'binkley.model'\n",
    "model12 = 'schaller.model'\n",
    "model13 = 'supplemental.model'\n",
    "model14 = 'wade.model'\n",
    "model15 = 'mosher_and_trantham.model'\n",
    "model16 = 'a_carey_and_sundberg.model'\n",
    "model17 = 'b_carey_and_sundberg.model'\n",
    "model18 = 'ravve.model'\n",
    "model19 = 'braun_chedron_rehahn_ritter_and_voit.model'\n",
    "model20 = 'koltzsenburg_maskos_and_nuyken.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "vocab built motha fucka!!!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "feed2vec(file1)\n",
    "w2v_train(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', 'electronic', 'structure', 'bonding', 'acids', 'bases', '1', 'introduction', 'organic', 'compounds', 'nomenclature', 'ph', '.', '.', '.', '2', 'al', '##ken', '##es', 'structure', 'nomenclature', 'introduction', 're', '.', '.', '.', '3', 'reactions', 'al', '##ken', '##es', '4', 'stereo', '##chemist', '##ry', 'arrangement', 'atoms', 'space', 'stereo', '.', '.', '.', '.', '.', '.', '750', '##9', 'figure', 'ionic', 'compound', 'na', '##cl', 'forms', 'electrons', 'so', '##d', '.', '.', '.', '75', '##10', 'lattice', 'energy', 'ie', 'energy', 'required', 'separate', 'mo', '##l', '.', '.', '.', '75', '##11', 'note', '75', '##12', 'due', 'ionic', 'compounds', 'lattice', 'energies', 'high', 'ions', '.', '.', '.', '75', '##13', 'ionic', 'solids', 'typically', 'go', 'solid', 'state', 'gas', 'stat', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '66', '##38', ',', 'dt', '##ype', ':', 'object']]\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "feed2vec(file2, tokenize=True)\n",
    "w2v_train(model2, last_model = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

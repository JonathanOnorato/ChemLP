{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step-by-step Deep Learning**\n",
    "\n",
    "This deep learning tutorial uses the Keras python library which is free and open-source. It acts as an interface for the TensorFlow library.\n",
    "The code is adapted from a machine learning mastery tutorial by Jason Brownlee PhD which is linked [here][]. The code is explained in more \n",
    "detail so a novice coder can understand what is happening.\n",
    "\n",
    "[here]: https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/ \"ML mastery\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first the python libraries need to be imported\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "import livelossplot\n",
    "plot_losses = livelossplot.PlotLossesKeras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading the Data**\n",
    "\n",
    "The dataset used for this tutorial is the Pima Indians Diabetes Database from \n",
    "the National Institute of Diabetes and Digestive and Kidney Diseases. It has 768 total data points. All paatients\n",
    "are females older than 21 and of Pima Indian heritage. The variables are all numerical. They are as \n",
    "follows:\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1 where 1 is \"tested positive for diabetes\")\n",
    "You can obtain the raw data [here][]. More info about the dataset can be found on this [webpage][]. Make sure to save it as a csv file, and split the text into different columns\n",
    "(Refer to this [link][] for how to do this).\n",
    "\n",
    "[here]: https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv \"Pima Indians Diabetes Data\"\n",
    "[webpage]: https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names \"Pima Data Details\"    \n",
    "[link]: https://support.microsoft.com/en-us/office/split-text-into-different-columns-with-the-convert-text-to-columns-wizard-30b14928-5550-41f5-97ca-7a3e9c363ed7 \"Split text into columnns\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "# the dataset needs to be in the same directory as your jupyter notebook\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Sequential Model Explained**\n",
    "\n",
    "Keras is an application programming interface (API) for using TensorFlow ie. it makes it easier for you to use the library,\n",
    "but it does have some limitations. The **Sequential model** is a plain stack of layers that only allows one input and one output.\n",
    "Layers can be added one-by-one or all at once. In model, the layers are added one-by-one. In contrast, model2 has the layers added all at-once \n",
    "A **Dense** layer has all the neurons fully connected while a dropout layer will randomly ignore a set of neurons. **Dropout** layers are \n",
    "generally used to prevent overfitting of the data. **Overfitting** is when the model fits the training data too well, so that it has more \n",
    "difficulty predicting additional data or make future predicitons.\n",
    "\n",
    "The first layer is the input layer. The size of it needs to fit the size of the data. In this case, there are 8 variables that the NN needs to \n",
    "consider. The number listed after **Dense** is the size of the output of that particular layer. The activation variable is the **activation function**.\n",
    "An **activation function** is a mathmatical equation that determines the output of each neuron, and it is applied to each neuron in its respective \n",
    "layer. There are common functions to use such as relu, linear, or sigmoid. Below is an example of a 2 layer NN with 3 input variables and 1 output. \n",
    "Each layer has 4 **nodes** or **neurons** which are units of computation that receive an input and use their **activation function** to calculate an ouput\n",
    "based on the **weights** and **biases** associated with the input (more on this later).  \n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1000/1*3fA77_mLNiJTSgZFhYnU0Q.png\" alt=\"Example of a 2 layer NN\" title=\"2-Layer NN\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "#model calls the Sequential object in keras\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu')) #input layer with 8 input variables. It uses relu activation function. It has an output of 12 \n",
    "model.add(Dense(8, activation='relu')) # 1st layer witch takes the output of the above layer as input. Uses reul activation function. Output of 8\n",
    "model.add(Dense(1, activation='sigmoid')) #Output layer takes the output of the above layer and makes the prediction of either 0 or 1. Activation function is sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([Dense(12, input_dim=8, activation='relu'), Dense(8, activation='relu'),Dense(1, activation='sigmoid')]) #same model as above just written horizontally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Compiling and Fitting the model**\n",
    "\n",
    "Compiling the model creates a Python object which builds the NN model that you defined previously. Once your model is built, it needs to be trained. \n",
    "This is done through optimization of the **weights**, which are learnable parameters of the NN applied to each input. At first, the NN \n",
    "randomizes the values for both **weight**, but as the NN training progresses, these parameters are updated and adjusted to\n",
    "achieve the desired output or value. **Weights** represent the \"strength\" of the connection between the input and output. So a larger weight means changing \n",
    "the input is more influential to the output. In contrast, a smaller weight results in little to no change in the output from changes in the input.\n",
    "Thus, weights influence how much a change in an input affects the output.When compiling the model, the loss function and type of optimizer need to be specified. \n",
    "A loss function is a method for discerning how effective a algorithm is at modeling a dataset, and its objective here is to minimize error. The optimizer specifys\n",
    "which optimization algorithm to use to vary the weights of the NN to achieve the optimal output. Finally, the metrics parameter just specifies what varaibles are being\n",
    "monitored. \n",
    "\n",
    "Next, the fit() function is called to train the model. It breaks up the training data into batches and iterates through the entire dataset a specified number of times or epochs. \n",
    "Below the batch_size is specified as 10, so the model is receiving 10 pieces of the dataset at a time. The number of epochs is specified as 150 which means the NN runs through\n",
    "the entire dataset 150 times. Also, the first two variables are the trianing ones from the dataset which were split up in a previous bit of code. As the fit functinos runs, \n",
    "it prints out the epoch number it is on, training step duration (time it takes to run through 1 batch of data), and accuracy (ie. did the model guess correctly?) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model ie this is creating the model that we defined previously\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 10.5586 - accuracy: 0.5690\n",
      "Epoch 2/150\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.1146 - accuracy: 0.6315\n",
      "Epoch 3/150\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5788 - accuracy: 0.6341\n",
      "Epoch 4/150\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 1.3087 - accuracy: 0.6445\n",
      "Epoch 5/150\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2453 - accuracy: 0.6393\n",
      "Epoch 6/150\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 1.1318 - accuracy: 0.6458\n",
      "Epoch 7/150\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.9394 - accuracy: 0.6536\n",
      "Epoch 8/150\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.9925 - accuracy: 0.6146\n",
      "Epoch 9/150\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.8863 - accuracy: 0.6432\n",
      "Epoch 10/150\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.8756 - accuracy: 0.6471\n",
      "Epoch 11/150\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.8295 - accuracy: 0.6654\n",
      "Epoch 12/150\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.7899 - accuracy: 0.6641\n",
      "Epoch 13/150\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.7510 - accuracy: 0.6706\n",
      "Epoch 14/150\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7572 - accuracy: 0.6693\n",
      "Epoch 15/150\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7194 - accuracy: 0.6615\n",
      "Epoch 16/150\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.6973 - accuracy: 0.6745\n",
      "Epoch 17/150\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.7050 - accuracy: 0.6615\n",
      "Epoch 18/150\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6825 - accuracy: 0.6836\n",
      "Epoch 19/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.6667\n",
      "Epoch 20/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6655 - accuracy: 0.6797\n",
      "Epoch 21/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7239 - accuracy: 0.6523\n",
      "Epoch 22/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6838 - accuracy: 0.6732\n",
      "Epoch 23/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6479 - accuracy: 0.7018\n",
      "Epoch 24/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6736 - accuracy: 0.6497\n",
      "Epoch 25/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6598 - accuracy: 0.6875\n",
      "Epoch 26/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6351 - accuracy: 0.6875\n",
      "Epoch 27/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6139 - accuracy: 0.6901\n",
      "Epoch 28/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.7148\n",
      "Epoch 29/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6249 - accuracy: 0.7018\n",
      "Epoch 30/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6341 - accuracy: 0.7057\n",
      "Epoch 31/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5915 - accuracy: 0.7148\n",
      "Epoch 32/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.7201\n",
      "Epoch 33/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6255 - accuracy: 0.6992\n",
      "Epoch 34/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6245 - accuracy: 0.7057\n",
      "Epoch 35/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6199 - accuracy: 0.7031\n",
      "Epoch 36/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5952 - accuracy: 0.7109\n",
      "Epoch 37/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6275 - accuracy: 0.6979\n",
      "Epoch 38/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5666 - accuracy: 0.7240\n",
      "Epoch 39/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6333 - accuracy: 0.6992\n",
      "Epoch 40/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5889 - accuracy: 0.7188\n",
      "Epoch 41/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6147 - accuracy: 0.7214\n",
      "Epoch 42/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5833 - accuracy: 0.7083\n",
      "Epoch 43/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5831 - accuracy: 0.7070\n",
      "Epoch 44/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5849 - accuracy: 0.7135\n",
      "Epoch 45/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5891 - accuracy: 0.6966\n",
      "Epoch 46/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6659 - accuracy: 0.6758\n",
      "Epoch 47/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5974 - accuracy: 0.7227\n",
      "Epoch 48/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6165 - accuracy: 0.7044\n",
      "Epoch 49/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5803 - accuracy: 0.7201\n",
      "Epoch 50/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5859 - accuracy: 0.7201\n",
      "Epoch 51/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5836 - accuracy: 0.7214\n",
      "Epoch 52/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5754 - accuracy: 0.7057\n",
      "Epoch 53/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5675 - accuracy: 0.7305\n",
      "Epoch 54/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5682 - accuracy: 0.7396\n",
      "Epoch 55/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5807 - accuracy: 0.7214\n",
      "Epoch 56/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6221 - accuracy: 0.7109\n",
      "Epoch 57/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5690 - accuracy: 0.7357\n",
      "Epoch 58/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.7279\n",
      "Epoch 59/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5997 - accuracy: 0.6979\n",
      "Epoch 60/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5666 - accuracy: 0.7383\n",
      "Epoch 61/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5669 - accuracy: 0.7174\n",
      "Epoch 62/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5530 - accuracy: 0.7357\n",
      "Epoch 63/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6178 - accuracy: 0.7148\n",
      "Epoch 64/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7461\n",
      "Epoch 65/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5571 - accuracy: 0.7383\n",
      "Epoch 66/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.6940\n",
      "Epoch 67/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5587 - accuracy: 0.7370\n",
      "Epoch 68/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5567 - accuracy: 0.7292\n",
      "Epoch 69/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.7370\n",
      "Epoch 70/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6075 - accuracy: 0.7135\n",
      "Epoch 71/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5840 - accuracy: 0.7109\n",
      "Epoch 72/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5506 - accuracy: 0.7292\n",
      "Epoch 73/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5435 - accuracy: 0.7474\n",
      "Epoch 74/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5513 - accuracy: 0.7396\n",
      "Epoch 75/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5602 - accuracy: 0.7396\n",
      "Epoch 76/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5377 - accuracy: 0.7526\n",
      "Epoch 77/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5867 - accuracy: 0.7174\n",
      "Epoch 78/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7474\n",
      "Epoch 79/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.7492 - accuracy: 0.6888\n",
      "Epoch 80/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5564 - accuracy: 0.7083\n",
      "Epoch 81/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5516 - accuracy: 0.7331\n",
      "Epoch 82/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5905 - accuracy: 0.7474\n",
      "Epoch 83/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5793 - accuracy: 0.7357\n",
      "Epoch 84/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7383\n",
      "Epoch 85/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5237 - accuracy: 0.7513\n",
      "Epoch 86/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7409\n",
      "Epoch 87/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6251 - accuracy: 0.7161\n",
      "Epoch 88/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.7357\n",
      "Epoch 89/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5920 - accuracy: 0.7227\n",
      "Epoch 90/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5682 - accuracy: 0.7292\n",
      "Epoch 91/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5285 - accuracy: 0.7552\n",
      "Epoch 92/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5300 - accuracy: 0.7409\n",
      "Epoch 93/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5361 - accuracy: 0.7435\n",
      "Epoch 94/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5540 - accuracy: 0.7331\n",
      "Epoch 95/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5585 - accuracy: 0.7279\n",
      "Epoch 96/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.7396\n",
      "Epoch 97/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5253 - accuracy: 0.7539\n",
      "Epoch 98/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5123 - accuracy: 0.7474\n",
      "Epoch 99/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5762 - accuracy: 0.7240\n",
      "Epoch 100/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5272 - accuracy: 0.7500\n",
      "Epoch 101/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5426 - accuracy: 0.7539\n",
      "Epoch 102/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5355 - accuracy: 0.7474\n",
      "Epoch 103/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5659 - accuracy: 0.7135\n",
      "Epoch 104/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5264 - accuracy: 0.7474\n",
      "Epoch 105/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5234 - accuracy: 0.7383\n",
      "Epoch 106/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5363 - accuracy: 0.7461\n",
      "Epoch 107/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5317 - accuracy: 0.7487\n",
      "Epoch 108/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5305 - accuracy: 0.7630\n",
      "Epoch 109/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5378 - accuracy: 0.7474\n",
      "Epoch 110/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5299 - accuracy: 0.7474\n",
      "Epoch 111/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5512 - accuracy: 0.7370\n",
      "Epoch 112/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5543 - accuracy: 0.7331\n",
      "Epoch 113/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5211 - accuracy: 0.7500\n",
      "Epoch 114/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5733 - accuracy: 0.7266\n",
      "Epoch 115/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5411 - accuracy: 0.7383\n",
      "Epoch 116/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5419 - accuracy: 0.7305\n",
      "Epoch 117/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5261 - accuracy: 0.7565\n",
      "Epoch 118/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7500\n",
      "Epoch 119/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5245 - accuracy: 0.7487\n",
      "Epoch 120/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5232 - accuracy: 0.7539\n",
      "Epoch 121/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5608 - accuracy: 0.7409\n",
      "Epoch 122/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5323 - accuracy: 0.7487\n",
      "Epoch 123/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5222 - accuracy: 0.7500\n",
      "Epoch 124/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7500\n",
      "Epoch 125/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5450 - accuracy: 0.7526\n",
      "Epoch 126/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5271 - accuracy: 0.7669\n",
      "Epoch 127/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5176 - accuracy: 0.7526\n",
      "Epoch 128/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5073 - accuracy: 0.7708\n",
      "Epoch 129/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5302 - accuracy: 0.7409\n",
      "Epoch 130/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5081 - accuracy: 0.7578\n",
      "Epoch 131/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5126 - accuracy: 0.7682\n",
      "Epoch 132/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5611 - accuracy: 0.7357\n",
      "Epoch 133/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5117 - accuracy: 0.7500\n",
      "Epoch 134/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5284 - accuracy: 0.7383\n",
      "Epoch 135/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5488 - accuracy: 0.7396\n",
      "Epoch 136/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4944 - accuracy: 0.7578\n",
      "Epoch 137/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5338 - accuracy: 0.7396\n",
      "Epoch 138/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5366 - accuracy: 0.7435\n",
      "Epoch 139/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5179 - accuracy: 0.7578\n",
      "Epoch 140/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5076 - accuracy: 0.7591\n",
      "Epoch 141/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5419 - accuracy: 0.7370\n",
      "Epoch 142/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5286 - accuracy: 0.7383\n",
      "Epoch 143/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5347 - accuracy: 0.7370\n",
      "Epoch 144/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5108 - accuracy: 0.7513\n",
      "Epoch 145/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5225 - accuracy: 0.7487\n",
      "Epoch 146/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.7409\n",
      "Epoch 147/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5259 - accuracy: 0.7565\n",
      "Epoch 148/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5241 - accuracy: 0.7487\n",
      "Epoch 149/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5036 - accuracy: 0.7539\n",
      "Epoch 150/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5108 - accuracy: 0.7461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22bfe810f88>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset ie \n",
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

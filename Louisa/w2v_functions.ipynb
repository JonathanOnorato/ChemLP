{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, PreTrainedTokenizer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    #removes unicodes left in text so model does not learn unicodes\n",
    "    text = re.sub('^\\\\\\\\u[\\d\\D]{4}|-|σ|→|\\\\\\\\xad', '', text)\n",
    "    # Remove a sentence if it is only one word long\n",
    "    if len(text) > 2:\n",
    "        return ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokens = tokenizer.tokenize(str(text), add_special_tokens=True)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatizer(text):\n",
    "    if text == None:\n",
    "        print(\"Excuse me but there is an unexpected None value from cleaning the text!\")\n",
    "        return \"\"\n",
    "    sent = []\n",
    "    tokens = []\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_2_vec(filepath, w2vmodel, tokenize=False, last_model=None):\n",
    "    global vocab\n",
    "    #Creates a pandas dataframe for the text data from a json file\n",
    "    df = pd.read_json(filepath)\n",
    "    \n",
    "    #Adds the column label text\n",
    "    df.columns = ['text']\n",
    "    \n",
    "    #Text clean function is applied to the pandas dataframe\n",
    "    df_clean = pd.DataFrame(df.text.apply(lambda x: clean_text(x)))\n",
    "    \n",
    "    #filters out all the None values in the cleaned dataset\n",
    "    #loc combines the operations in brackets into one single operation to avoid chaining indexes operations together\n",
    "    #copy explicitly tells pandas to make copy when creating master_of_none\n",
    "    #this is so later on only the copy is modified and there is no confusion between the copy and the original\n",
    "    master_of_none = df_clean.loc[df_clean.text.notnull()].copy()\n",
    "    \n",
    "    #Lemmatizer function is applied to cleaned text with the none values removed\n",
    "    master_of_none[\"text_lemmatized\"] =  master_of_none.apply(lambda x: lemmatizer(x['text']), axis=1)\n",
    "    \n",
    "    master_of_none['text_lemmatize_clean'] = master_of_none['text_lemmatized'].str.replace('-PRON-', '')\n",
    "\n",
    "    sentences = [row.split() for row in master_of_none['text_lemmatize_clean']]\n",
    "    word_freq = defaultdict(int)\n",
    "    for sent in sentences:\n",
    "    #tokens = tokenizer(sent)\n",
    "        for i in sent:\n",
    "            word_freq[i] += 1\n",
    "    len(word_freq)\n",
    "\n",
    "    if last_model is not None:\n",
    "        \n",
    "        #Previously updated model is loaded\n",
    "        w2v_model = Word2Vec.load(last_model)  \n",
    "        \n",
    "        #model vocabulary is updated\n",
    "        w2v_model.build_vocab(sentences, update=True)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        # min_count: minimum number of occurrences of a word in the corpus to be included in the model.\n",
    "        # window: the maximum distance between the current and predicted word within a sentence.\n",
    "        # size: the dimensionality of the feature vectors\n",
    "        # workers: the number of cores your computer has\n",
    "        w2v_model = Word2Vec(min_count=50,\n",
    "                             window=5,\n",
    "                             size=400,\n",
    "                             workers=2)\n",
    "        #the new model's vocabulary is built \n",
    "        w2v_model.build_vocab(sentences)\n",
    "\n",
    "    # train word vectors\n",
    "    #returns the number of words in the vocab and the number of words in the corpus\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)\n",
    "    \n",
    "    #either the new or updated version of the w2v model is saved\n",
    "    w2v_model.save(w2vmodel)\n",
    "    return print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_word = ['alcohol', 'ketone', 'alkene', 'carbon', 'proton', 'polymer', 'acid', 'oxidize', 'anion', 'electrophile', 'polar', 'positive', 'mechanism', 'resonance', 'synthesis', 'isomer', 'heat', 'aromatic', ]\n",
    "second_word = ['hydroxyl', 'carbonyl', 'alkyne', 'nitrogen', 'hydrogen', 'chain', 'base', 'reduce', 'cation', 'nucleophile', 'nonpolar', 'negative', 'atom', 'solvent', 'electron', 'reaction', 'bond', 'equilibrium']\n",
    "\n",
    "\n",
    "def cosine_sim_saver(w2vmodel, first_word, second_word):\n",
    "    \n",
    "    global w2v_data\n",
    "    \n",
    "    w2v_model = Word2Vec.load(w2vmodel)\n",
    "    \n",
    "    w2v_data = []\n",
    "    \n",
    "    \n",
    "    for word1, word2 in zip(first_word, second_word):\n",
    "        try:\n",
    "        #synonym, antonym, or neutral pairs\n",
    "            cos_sim = w2v_model.wv.similarity(word1, word2)\n",
    "            w2v_data.append(cos_sim)\n",
    "        except KeyError:\n",
    "            cos_sim = 0 \n",
    "            w2v_data.append(cos_sim)\n",
    "            print(\"One word was not in the vocabulary\")\n",
    "   \n",
    "    try:\n",
    "        top10 = w2v_model.wv.most_similar(positive=['carbon'])\n",
    "        w2v_data.append(top10)\n",
    "    except KeyError:\n",
    "        top10 = 0\n",
    "        print(\"carbon is not in the vocabulary\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    return print(\"Data collection complete!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_headers = ['model_name', 'alcohol + hydroxyl', 'ketone + carbonyl', 'alkene + alkyyne', 'carbon + nitrogen', 'proton + hydrogen', 'polymer + chain', 'acid + base', 'oxidize + reduce', 'anion + cation', 'electrophile + nucleophile', 'polar + nonpolar', 'positive + negative', 'mechanism + atom', 'resonance + solvent', 'synthesis + electron', 'isomer + reaction', 'heat + bond', 'aromatic + equilibrium', 'Top 10 Carbon']\n",
    "\n",
    "def data_saver(excel_file, my_headers, new_file=None):\n",
    "    if new_file is not None:\n",
    "        with open(excel_file, 'w') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(my_headers)\n",
    "            csvwriter.writerow(w2v_data)\n",
    "    else:\n",
    "        with open(excel_file, 'a+', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(w2v_data)\n",
    "    return print(\"Data saved as CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_grapher(w2vmodel):\n",
    "    \"Create TSNE model and plot it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    model = Word2Vec.load(w2vmodel)\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(18, 18)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'ChemLibre_JSONS/Basic_Principles_of_Organic_Chemistry_Roberts_and_Caserio.json'\n",
    "file2 = 'ChemLibre_JSONS/Bruice_Map.json'\n",
    "file3 = 'ChemLibre_JSONS/Catalytic_Asymmetric_Synthesis_Punniyamurthy.json'\n",
    "file4 = 'ChemLibre_JSONS/Environmental_Chemistry.json'\n",
    "file5 = 'ChemLibre_JSONS/How_to_be_a_Successful_Organic_Chemist_Sandtorv.json'\n",
    "file6 = 'ChemLibre_JSONS/Logic_of_Organic_Synthesis_Rao.json'\n",
    "file7 = 'ChemLibre_JSONS/Organic_Chemistry_A _Carbonyl_Early_Approach_McMichael.json'\n",
    "file8 = 'ChemLibre_JSONS/Organic_Chemistry_Lab_Techniques_Nichols.json'\n",
    "file9 = 'ChemLibre_JSONS/Organic_Chemistry_with_a_Biological_Emphasis_Soderberg.json'\n",
    "file10 = 'ChemLibre_JSONS/Polymer_Chemistry.json'\n",
    "file11 = 'ChemLibre_JSONS/Radical_Reactions_of_Carbohydrates_Binkley.json'\n",
    "file12 = 'ChemLibre_JSONS/Schaller_Polymer.json'\n",
    "file13 = 'ChemLibre_JSONS/Supplemental_Modules.json'\n",
    "file14 = 'ChemLibre_JSONS/Wade_Map.json'\n",
    "file15 = 'Springer_PDF/Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json'\n",
    "file16 = 'Springer_PDF/Advanced_Organic_Chemistry_Part_A_Structure_and_Mechanisms_by_Carey_and_Sundberg.json'\n",
    "file17 = 'Springer_PDF/Advanced_Organic_Chemistry_Part_B_Reactions_and_Synthesis_by_Carey_and_Sundberg.json'\n",
    "file18 = 'Springer_PDF/Principles_of_Polymer_Chemistry_by_Ravve.json'\n",
    "file19 = 'Springer_PDF/Polymer_Synthesis_Theory_and_Practice_by_Braun_Cherdron_Rehahn_Ritter_and_Voit.json'\n",
    "file20 = 'Springer_PDF/Polymer_Chemistry_by_Koltzsenburg_Maskos_and_Nuyken.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = 'robers_and_caserio.model'\n",
    "model2 = 'bruice.model'\n",
    "model3 = 'punniyamurthy.model'\n",
    "model4 = 'environmental.model'\n",
    "model5 = 'sandtorv.model'\n",
    "model6 = 'rao.model'\n",
    "model7 = 'mcmichael.model'\n",
    "model8 = 'nichols.model'\n",
    "model9 = 'soderberg.model'\n",
    "model10 = 'polymer.model'\n",
    "model11 = 'binkley.model'\n",
    "model12 = 'schaller.model'\n",
    "model13 = 'supplemental.model'\n",
    "model14 = 'wade.model'\n",
    "model15 = 'mosher_and_trantham.model'\n",
    "model16 = 'a_carey_and_sundberg.model'\n",
    "model17 = 'b_carey_and_sundberg.model'\n",
    "model18 = 'ravve.model'\n",
    "model19 = 'braun_chedron_rehahn_ritter_and_voit.model'\n",
    "model20 = 'koltzsenburg_maskos_and_nuyken.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-bff029d3cf15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mword_2_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mword_2_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mword_2_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mword_2_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mword_2_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile17\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel17\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-96c4f2c0267f>\u001b[0m in \u001b[0;36mword_2_vec\u001b[1;34m(filepath, w2vmodel, tokenize, last_model)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#Creates a pandas dataframe for the text data from a json file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#Adds the column label text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Chem_NLP\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Chem_NLP\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Chem_NLP\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Chem_NLP\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"series\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Chem_NLP\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Chem_NLP\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1087\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m-> 1089\u001b[1;33m                 \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1090\u001b[0m             )\n\u001b[0;32m   1091\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "word_2_vec(file1, model1)\n",
    "word_2_vec(file2, model2)\n",
    "word_2_vec(file3, model3)\n",
    "word_2_vec(file4, model4)\n",
    "word_2_vec(file5, model5)\n",
    "word_2_vec(file6, model6)\n",
    "word_2_vec(file7, model7)\n",
    "word_2_vec(file8, model8)\n",
    "word_2_vec(file9, model9)\n",
    "word_2_vec(file10, model10)\n",
    "word_2_vec(file11, model11)\n",
    "word_2_vec(file12, model12)\n",
    "word_2_vec(file13, model13)\n",
    "word_2_vec(file14, model14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "word_2_vec(file15, model15)\n",
    "word_2_vec(file16, model16)\n",
    "word_2_vec(file17, model17)\n",
    "word_2_vec(file18, model18)\n",
    "word_2_vec(file19, model19)\n",
    "word_2_vec(file20, model20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "word_2_vec(file18, model18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model1, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model2, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "carbon is not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model3, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model4, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "carbon is not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model5, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "carbon is not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model6, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model7, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "carbon is not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model8, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u03b1' in position 293: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-5fc662982e4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcosine_sim_saver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_saver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all_texts.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_headers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-8936eaeaa26c>\u001b[0m in \u001b[0;36mdata_saver\u001b[1;34m(excel_file, my_headers, new_file)\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mcsvwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mcsvwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_headers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mcsvwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexcel_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Chem_NLP\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u03b1' in position 293: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model9, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "carbon is not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model10, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "carbon is not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model11, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "carbon is not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model12, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model13, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model14, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model15, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model16, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model17, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model18, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model19, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "One word was not in the vocabulary\n",
      "Data collection complete!\n",
      "Data saved as CSV\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_saver(model20, first_word, second_word)\n",
    "data_saver('all_texts.csv', my_headers, new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

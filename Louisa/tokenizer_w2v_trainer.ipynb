{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    #removes unicodes left in text so model does not learn unicodes\n",
    "    text = re.sub('^\\\\\\\\u[\\d\\D]{4}|-|σ|→|\\\\\\\\xad', '', text)\n",
    "    # Remove a sentence if it is only one word long\n",
    "    if len(text) > 2:\n",
    "        return ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokens = tokenizer.tokenize(str(text), add_special_tokens=True)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatizer(text):\n",
    "    if text == None:\n",
    "        print(\"Daisy, daisy, sour cream!!!\")\n",
    "        return \"\"\n",
    "    sent = []\n",
    "    tokens = []\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default value of display.max_rows is 10 i.e. at max 10 rows will be printed.\n",
    "# Set it None to display all rows in the dataframe\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#types = df_clean.apply(lambda x: type(x['text']), axis=1)\n",
    "\n",
    "df = pd.read_json('ChemLibre_JSONS/Basic_Principles_of_Organic_Chemistry_Roberts_and_Caserio.json')\n",
    "df.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean_no_none = df_clean[df_clean.text.notnull()]\n",
    "\n",
    "#df = pd.read_json('ChemLibre_JSONS/Wade_Map.json')\n",
    "#df.columns = ['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame(df.text.apply(lambda x: clean_text(x)))\n",
    "#filters out all the None values in the cleaned dataset\n",
    "#loc combines the operations in brackets into one single operation to avoid chaining indexes operations together\n",
    "#copy explicitly tells pandas to make copy when creating master_of_none\n",
    "#this is so later on only the copy is modified and there is no confusion between the copy and the original\n",
    "master_of_none = df_clean.loc[df_clean.text.notnull()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_of_none[\"text_tokenize\"] =  master_of_none.apply(lambda x: tokenizer(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_of_none[\"text_lemmatized_tokens\"] =  master_of_none.apply(lambda x: lemmatizer(x['text_tokenize']), axis=1)\n",
    "master_of_none[\"text_lemmatized\"] =  master_of_none.apply(lambda x: lemmatizer(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_of_none[\"text_lemmatized_tokens\"] =  master_of_none.apply(lambda x: tokenizer(x['text_lemmatized']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean['text_lemmatize_clean'] = df_clean['text_lemmatize'].str.replace('-PRON-', '')\n",
    "#master_of_none['text_lemmatize_clean'] = master_of_none['text_lemmatized_tokens'].str.replace('-PRON-', '')\n",
    "\n",
    "#sentences = [for row in master_of_none['text_lemmatized_tokens']]\n",
    "word_freq = defaultdict(int)\n",
    "for row in master_of_none['text_lemmatized_tokens']:\n",
    "    for i in row:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)\n",
    "\n",
    "# min_count: minimum number of occurrences of a word in the corpus to be included in the model.\n",
    "# window: the maximum distance between the current and predicted word within a sentence.\n",
    "# size: the dimensionality of the feature vectors\n",
    "# workers: I know my system is having 4 cores,\n",
    "\n",
    "w2v_model = Word2Vec(min_count=50,\n",
    "                     window=5,\n",
    "                     size=400,\n",
    "                     workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model = Word2Vec.load(\"trial_19.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line of code to prepare the model vocabulary\n",
    "w2v_model.build_vocab(master_of_none['text_lemmatized_tokens'])\n",
    "#w2v_model.build_vocab(sentences, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word vectors\n",
    "#returns the number of words in the vocab and the number of words in the corpus\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model = Word2Vec.load(\"No_tokenize/trial_1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the model\n",
    "w2v_model.wv.most_similar(positive=['chemistry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('aromatic', 'equilibrium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('alcohol', 'hydroxyl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('ketone', 'carbonyl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('alkene', 'alkyne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('acid', 'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('oxidize', 'reduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('anion', 'cation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('mechanism', 'atom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('resonance', 'solvent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('synthesis', 'electron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"test.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we do not plan to train the model any further, \n",
    "# we are calling init_sims(), which will make the model much more memory-efficient\n",
    "#w2v_model.init_sims(replace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

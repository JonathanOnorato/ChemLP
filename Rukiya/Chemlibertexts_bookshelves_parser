# First let's import all the neccesery libararies and modules
import pandas as pd
from lxml import html
import selenium
from selenium import webdriver
import random
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
import json
from selenium.common.exceptions import StaleElementReferenceException 
from selenium.common.exceptions import NoSuchElementException

def chemlibertexts_bookschelves_parser(url):
    """"This function takes one variable 'url' a Bookshelves url from Chem.Libretexts)""" 
    driver = webdriver.Chrome()
    driver.implicitly_wait(random.randint(1,30)) 
    driver.get(url)
    driver.implicitly_wait(random.randint(1,100))
    
    # driver collects hyperlinks of books in the bookshelves.
    # in the bookshelves, books are distinguished by putting "Book: xxxx" on their title from other
    # types of materials, such as maps, excercise, etc.
    # we also collect the name of the bookshelves so that we can use it as file name to store the data
    
    book_links = []
    book_titles_shortoverviews = []
    bookshelves_name = driver.find_element(By.XPATH, '//*[@id="title"]').text.strip()
    main_content_container = driver.find_element(By.CLASS_NAME, 'mt-content-container')
    book_link_container = main_content_container.find_elements(By.TAG_NAME, 'a')
    for link in book_link_container:
        if 'Book:' in link.get_attribute('title'):
            book_titles_shortoverviews.append(link.get_attribute('title').strip()) 
            book_links.append(link.get_attribute('href')) 
        else:
            pass
    
    # Next let's collect the hyperlinks of book chapters. 
    # Depending on the style of books in the chemlibertexts website we need to use different webelements as seen blow;
    # In order to avoid the 'NoSuchElementException' error 'try and except'conditions are used. 
    # the first "try and except" is for books that we need to click to the table of contens page, requires two clicks and all chapters are listed under table of contens 
    # the second "try and except" is for books (which is more common in the website) that have main-chapter-hyperlinks on the book homepage
    # then we need to click mainchapter to get subchapterhyperlikns 
    
    book_titles = []
    chapter_links = []
    chapter_titles = []
   
    # first the driver finds the title of the book and stores under variable 'book_titles' 
    #for i in range(len(book_titles)):
   
    for link in book_links:
        driver.get(link)
        driver.page_source
        book_titles. append(driver.find_element(By.XPATH, '//*[@id="title"]').text.strip())
        try:
            driver.find_element(By.CLASS_NAME, 'mt-icon-next-article').click()
            driver.page_source
            driver.find_element(By.XPATH, '//*[@title="Table of Contents"]').click()
            chapter_content_container = driver.find_element(By.CLASS_NAME, 'mt-content-container')
            chapter_link_container = chapter_content_container.find_elements(By.TAG_NAME,'a')
            chapter_links = (link.get_attribute('href') for link in chapter_link_container)
            chapter_titles = (link.text.strip() for link in chapter_link_container)
        except:
            NoSuchElementException
            
        try:
            driver.find_element(By.XPATH, '//*[@id=title="Front Matter"]').click()
            driver.page_source
            driver.find_element(By.XPATH, '//*[@title="Table of Contents"]').click()
            chapter_content_container = driver.find_element(By.CLASS_NAME, 'mt-content-container')
            chapter_link_container = chapter_content_container.find_elements(By.TAG_NAME,'a')
            for link in chapter_link_container:
                if "Back Matter" in link.text.strip():
                    pass
                elif "Index" in link.text.strip():
                    pass
                else:
                    chapter_links.append(link.get_attribute('href'))
                    chapter_titles.append(link.text.strip())       
        except:
            NoSuchElementException
            
            
        try:
            chapter_container = driver.find_element(By.CLASS_NAME, 'mt-sortable-listings-container')
            chapter_link_container = chapter_container.find_elements(By.TAG_NAME,'a')
            for chapter_link in chapter_link_container:
                chapter_links.append(chapter_link.get_attribute('href'))
                chapter_titles.append(chapter_link.get_attribute('title'))
        except:
            NoSuchElementException
        
   # using the chapter hyperlinks collected let's collect subchapter lists
    subchapter_links = []
    subchapter_titles = []                          
    for link in chapter_links:
        driver.get(link)
        driver.page_source
        try:
            subchapter_container = driver.find_element(By.CLASS_NAME, 'noindex')
            subchapter_link_container = subchapter_container.find_elements(By.TAG_NAME,'a')
            for link in subchapter_link_container:
                subchapter_links.append(link.get_attribute('href'))
                subchapter_titles.append(link.get_attribute('title'))
        except:
            NoSuchElementException
    
    #finally we add chapter links and subchapter links and also remove any duplicates.
    # and loop through the total hyperlinks to parse their text content and add that data to the list of chapter-titles
    
    total_chapter_titles = list(dict.fromkeys(chapter_titles + subchapter_titles))
    total_chapter_links = list(dict.fromkeys(chapter_links + subchapter_links))                                                     
                                
    book_contents = total_chapter_titles 
    
    for link in total_chapter_links:
        driver.get(link)
        driver.page_source
        try:
            chapter_container = driver.find_element(By.CLASS_NAME, 'mt-content-container')
            subchapter_text_container = chapter_container.find_elements(By.TAG_NAME,'p') 
            for subchap in subchapter_text_container:
                if subchap.text.strip() != '':
                    book_contents.append(subchap.text.strip())
                else:
                    pass
        except:
            EC, NoSuchElementException, StaleElementReferenceException 
            
    #store the data as JSON file with bookshelves' name as filename
    
    data = book_contents
    
    filename = bookshelves_name
    with open(filename, "w") as outfile:
            json.dump(data, outfile)
    # the function returns the list of book titles and the number of total chapter(main and sub) hyperlinks     
    return book_titles,len(total_chapter_links), driver.close()

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT data cleaner and tokenizer\n",
    "\n",
    "we use NLTK, Regex, BeautifulSoup, inflect to clean the parsed text data (json file), tokenize them into sentences and restore them as JSON file.\n",
    "Resources used to create this code are as following:\n",
    "* <a href=\"https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\" target=\"_black\" >  Text processing </a>\n",
    "*  <a href=\"https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/\" target =\"_black\" >Tokenization </a>\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Step 1:</code> Importing all the neccesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, string, unicodedata\n",
    "import inflect\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "import json\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Step 2:</code> Openning, premilinary cleaning, and tokenizing the text data into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_clean_tokenize(filepath):\n",
    "    \"\"\" filepath is the path to the json file to be openned. This function opens the file, do some basic cleaning (makes text lowercase, remoeves  and removes unwanted symboles and tokenize the data in to sentences\n",
    "\"\"\"\n",
    "    with open(filepath) as json_file:\n",
    "        corpus = json.load(json_file)\n",
    "        #corpus = pd.read_json(json_file, orient ='index') \n",
    "    corpus = str(corpus)\n",
    "    corpus = BS(corpus, 'lxml').text #HTML decoding. BeautifulSoup's text attribute will return a string stripped of any HTML tags and metadata.\n",
    "    corpus = corpus.lower()\n",
    "    corpus = re.sub(r'\\w*\\d\\w*', '', corpus)\n",
    "    corpus = re.sub(r'http\\S+', '', corpus) # removes url\n",
    "    corpus = re.compile('[/(){}\\[\\]\\|@;#:]').sub('', corpus) #replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    corpus = re.compile('(\\$[a-zA-Z_][0-9a-zA-Z_]*)').sub('', corpus) #remove symbols from text.\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle') # tokenizes data into sentences\n",
    "    corpus = tokenizer.tokenize(corpus)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Step 3:</code> Further cleaning (remove punctuations, numbers, and stop words) the tokenized data in setp 2 for to use in Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(corpus):\n",
    "    \"\"\"Remove punctuation from list of tokenized corpus\"\"\"\n",
    "    new_corpus = []\n",
    "    for token in corpus:\n",
    "        new_token = re.sub(r'[^\\w\\s]', '', token)\n",
    "        if new_token != '':\n",
    "            new_corpus.append(new_token)\n",
    "    return new_corpus\n",
    "\n",
    "def replace_numbers(corpus):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_corpus = []\n",
    "    for token in corpus:\n",
    "        if token.isdigit():\n",
    "            new_token = p.number_to_words(token)\n",
    "            new_corpus.append(new_token)\n",
    "        else:\n",
    "            new_corpus.append(token)\n",
    "    return new_corpus\n",
    "\n",
    "def remove_stopwords(corpus):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_corpus = []\n",
    "    for token in corpus:\n",
    "        if token not in stopwords.words('english'):\n",
    "            new_corpus.append(token)\n",
    "    return new_corpus\n",
    "\n",
    "def normalize(corpus):\n",
    "    corpus = remove_punctuation(corpus)\n",
    "    corpus = replace_numbers(corpus)\n",
    "    corpus = remove_stopwords(corpus)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code> Step 4:</code> Let's run the above two functions as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'Basic Principles of Organic Chemistry_Roberts and Caserio'\n",
    "corpus = open_clean_tokenize(filepath)\n",
    "tokens = normalize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = normalize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatization\n",
    "\n",
    "We can further process the cleanned and tokenized data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(tokens):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    tokens = normalize(corpus)\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stem = stemmer.stem(token)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(tokens):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_and_lemmatize(tokens):\n",
    "    stems = stem_words(tokens)\n",
    "    lemmas = lemmatize_verbs(tokens)\n",
    "    return stems, lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = stem_and_lemmatize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Scraper for Books from Chem.Libretexts\n",
    "\n",
    "## Overview\n",
    "\n",
    "The following text scraper <code> chemlibertexts_book_parser (ul) </code> extracts texts from books in the <a href ='https://chem.libretexts.org/Bookshelves' target = \"_black\"> Chem.Libretexts</a> website and store them as a JSON file with under the name of the book that one want to scrape.\n",
    "    \n",
    "The function one variable <code> url </code> which can be any book webpage from Chemlibertexts. Although most books have the same html structure, there are couple exceptions. Therefore depending on the type of the book different webelements are used to extract hyperlinks.\n",
    "\n",
    "Also, note that for webdriver, we used chrome. Please adjust it according to the type of webdriver you prefer using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's import all the neccesery libararies and modules\n",
    "\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import random\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "from selenium.common.exceptions import StaleElementReferenceException \n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def chemlibertexts_book_parser(url):\n",
    "    \"\"\"\"This function takes one variable 'url' (a book chapter url from Chem.Libretexts)\"\"\" \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(random.randint(1,30)) \n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(random.randint(1,100))\n",
    "    \n",
    "    # first the driver finds the title of the book and stores under variable 'book_title' which we use later as a filename to store the parsed data\n",
    "    book_title = driver.find_element(By.XPATH, '//*[@id=\"title\"]').text.strip()\n",
    "    \n",
    "    # Next let's collect the hyperlinks of book chapters. \n",
    "    # Depending on the style of books in the chemlibertexts website we need to use different webelements as seen blow;\n",
    "    # In order to avoid the 'NoSuchElementException' error 'try and except'conditions are used. \n",
    "    # the first \"try and except\" is for books that we need to click to the table of contens page, requires two clicks and all chapters are listed under table of contens \n",
    "    # the second \"try and except\" is for books (which is more common in the website) that have main-chapter-hyperlinks on the book homepage\n",
    "    # then we need to click mainchapter to get subchapterhyperlikns \n",
    "    \n",
    "    chapter_titles = []\n",
    "    chapter_links = [] \n",
    "    chapter_summary = []\n",
    "    try:\n",
    "        chapter_container = driver.find_element(By.CLASS_NAME, 'mt-sortable-listings-container')\n",
    "        chapter_link_container = chapter_container.find_elements(By.TAG_NAME,'a')\n",
    "        for chapter_link in chapter_link_container:\n",
    "            if 'Front Matter' in chapter_link.get_attribute('title'):\n",
    "                pass\n",
    "            elif 'Back Matter' in chapter_link.get_attribute('title'):\n",
    "                pass\n",
    "    \n",
    "            else:\n",
    "                chapter_links.append(chapter_link.get_attribute('href'))\n",
    "                chapter_titles.append(chapter_link.get_attribute('title'))\n",
    "    except:\n",
    "        NoSuchElementException \n",
    "        \n",
    "    try:\n",
    "        driver.find_element(By.CLASS_NAME, 'mt-icon-next-article').click()\n",
    "        driver.page_source\n",
    "        driver.find_element(By.XPATH, '//*[@title=\"Table of Contents\"]').click()\n",
    "        chapter_content_container = driver.find_element(By.CLASS_NAME, 'mt-content-container')\n",
    "        chapter_link_container = chapter_content_container.find_elements(By.TAG_NAME,'a')\n",
    "        chapter_links = [link.get_attribute('href') for link in chapter_link_container]\n",
    "        chapter_titles = [link.text.strip() for link in chapter_link_container]\n",
    "        chapter_summary_container = chapter_container.find_elements(By.CLASS_NAME, 'summary')\n",
    "        chapter_summary = [summary.text.strip() for summary in chapter_summary_container]\n",
    "    except:\n",
    "        NoSuchElementException\n",
    "    \n",
    "            \n",
    "    print('Name of book', book_title)\n",
    "    \n",
    "     # Following for loop go through the chapter_links collected above and looks for subchapter hyperlinks\n",
    "    # And also looks for overviews. Aagain, to avoid the 'NoSuchElementException' following 'try and except'condition is used\n",
    "    \n",
    "    subchapter_links = []\n",
    "    subchapter_titles = []\n",
    "    subchapter_link_container = []\n",
    "    \n",
    "    for chapter in list(dict.fromkeys(chapter_links)):\n",
    "        driver.get(chapter)\n",
    "        driver.page_source  \n",
    "        try:\n",
    "            subchapter_container = driver.find_element(By.CLASS_NAME, 'noindex')\n",
    "            subchapter_link_container = subchapter_container.find_elements(By.TAG_NAME,'a')\n",
    "            for link in subchapter_link_container:\n",
    "                subchapter_links.append(link.get_attribute('href'))\n",
    "                subchapter_titles.append(link.get_attribute('title'))\n",
    "        except:\n",
    "            NoSuchElementException\n",
    "            \n",
    "     # After collectong all chapter and subchapter links, let's combine all hyperlinks\n",
    "    # Before looping through the links let's remove duplicates;\n",
    "    # because for some books, all chapter hyperlinks including subchapters are given in one page;\n",
    "    # so that when we collect subchapters, we endup having the same list for two times!\n",
    "    \n",
    "    total_chapter_titles = list(dict.fromkeys(chapter_titles + subchapter_titles))\n",
    "    total_chapter_links = list(dict.fromkeys(chapter_links + subchapter_links))\n",
    "    \n",
    "    \n",
    "    # in order to avoid duplicated looping over chapterlinks for the first types of book the following if conditions are applied\n",
    "    \n",
    "    book_contents = total_chapter_titles\n",
    "    \n",
    "    if len(chapter_links) > len(subchapter_links):\n",
    "        for link in list(dict.fromkeys(chapter_links)):\n",
    "            driver.get(link)\n",
    "            driver.page_source\n",
    "            chapter_container = driver.find_element(By.CLASS_NAME, 'mt-content-container')\n",
    "            subchapter_text_container = chapter_container.find_elements(By.TAG_NAME,'p') \n",
    "            for subchap in subchapter_text_container:\n",
    "                 if subchap.text.strip() != '':\n",
    "                    book_contents.append(subchap.text.strip())\n",
    "    else:\n",
    "        for link in total_chapter_links:\n",
    "            driver.get(link)\n",
    "            driver.page_source\n",
    "            chapter_container = driver.find_element(By.CLASS_NAME, 'mt-content-container')\n",
    "            subchapter_text_container = chapter_container.find_elements(By.TAG_NAME,'p') \n",
    "            for subchap in subchapter_text_container:\n",
    "                if subchap.text.strip() != '':\n",
    "                    book_contents.append(subchap.text.strip())\n",
    "            # tried to eliminate the texts containing url or Problem MP, but seemed do not work\n",
    "                #elif subchap.text.strip().find(\"https://\") != -1:\n",
    "                  #  pass\n",
    "                #elif subchap.text.strip().find(\"Problem MP\") != -1:\n",
    "                 #   continue\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "    \n",
    "    # in oder to have more clean data, let's put all text's in a single list \n",
    "    # and store that list with the name of the book\n",
    "    \n",
    "    data = book_contents\n",
    "    filename = book_title\n",
    "    with open(filename, \"w\") as outfile:\n",
    "            json.dump(data, outfile)\n",
    "            \n",
    "    print('Number of total chapters is', len(total_chapter_links))\n",
    "    return  driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

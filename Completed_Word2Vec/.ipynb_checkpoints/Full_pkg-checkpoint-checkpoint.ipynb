{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:53:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 11:53:56: built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The purpose of this program is to read a local corpus of .json and .txt files locally and generate a list of tokens using BERT. The BERT tokens are then \n",
    "embedded as vectors. The program writes the final dictionary of words and their corresponding embeddings into a .json file locally. Sometimes it produces\n",
    "an attribute error, stating that str objects have no attribute dump (from json import), however restarting the kernal and running all cells fixes this. \n",
    "\"\"\"\n",
    "\n",
    "#need to double check if we need these if they're already in Lousia's functions\n",
    "import re, string \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, PreTrainedTokenizer\n",
    "import csv\n",
    "import glob \n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "#louisa's functions imported from local download as .py, added lines = true to fix value error when reading json into df\n",
    "import Louisa_w2v_functions as w2v_functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def super_list_maker(separate = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function prompts the user for a local folder/corpus location where the .json and .txt files are located. It then BERT tokenizes them and reads the \n",
    "    tokens into a list or list of lists depending on the default variable separate(true => list of lists vs false => single token list). In either case, the final list is then \n",
    "    returned. For users on windows, the filepath format leading to the corpus would be: C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files for example\n",
    "    \"\"\"\n",
    "    \n",
    "    initPath = input(\"Enter a file location\")\n",
    "    bookType = input(\"Enter a data type(.json please)\")\n",
    "    token_list = []\n",
    "    final_list = []\n",
    "    sep_token_list_of_lists = []\n",
    "    if bookType:\n",
    "        path = initPath + \"\\*\" + bookType\n",
    "    else:\n",
    "        path = initPath\n",
    "    raw_path = r\"{}\".format(path)\n",
    "    bookList = glob.glob(raw_path)\n",
    "    \n",
    "    yesOrNo = input(\"Is there an additional file type? (y or n) \")\n",
    "    if yesOrNo == \"y\":\n",
    "        bookType2 = input(\"Enter a second data type(.txt) \")\n",
    "        path2 = initPath + \"\\*\" + bookType2\n",
    "        raw_path2 = r\"{}\".format(path2)\n",
    "        bookList2 = glob.glob(raw_path2)\n",
    "        for bookSite in bookList2:\n",
    "            bookList.append(bookSite)\n",
    "    \n",
    "    bookCount = len(bookList)\n",
    "    print(\"bookCount is \", bookCount)\n",
    "    print(\"book list is \", bookList)\n",
    "\n",
    "    \n",
    "    for i in range(bookCount): #(len(bookList)):\n",
    "        sep_book_list = w2v_functions.feed2vec(bookList[i], tokenize = True)\n",
    "        sep_token_list_of_lists.append(sep_book_list[0])\n",
    "        print(\"token list of book \", i+1, \"is \", sep_book_list[0])\n",
    "        print(\"tokens in book \", i+1, \"is\", len(sep_book_list[0]))\n",
    "        print()\n",
    "        for word in sep_book_list[0]:\n",
    "            token_list.append(word)  #use true here\n",
    "        print(\"data type of token_list[\", i+1, \"]\", type(token_list[i]))\n",
    "\n",
    "    print()\n",
    "    print(\"length of vocab \", len(token_list))\n",
    "    \n",
    "    if separate == False:\n",
    "        print(\"final list length/token count \", len(token_list))\n",
    "        print (\"all tokens \", token_list)\n",
    "        return token_list\n",
    "    else:\n",
    "        print(\"list of all tokens (list of list form) \", sep_token_list_of_lists)\n",
    "        return sep_token_list_of_lists\n",
    "    print()\n",
    "\n",
    "\n",
    "#local book path = C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files   this format is what works, then use .json as type  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'super_list_maker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-da8f537f3b6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mvecs_and_words_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mvecs_and_words_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_returner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mjson\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvecs_and_words_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-da8f537f3b6d>\u001b[0m in \u001b[0;36membedding_returner\u001b[1;34m(list_of_lists, minC)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mlist_of_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper_list_maker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseparate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_of_lists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlist_of_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0msaveName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter a name for the model saved: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'super_list_maker' is not defined"
     ]
    }
   ],
   "source": [
    "#add default variables to support variable in w2v_train!\n",
    "#not sure if it's bad form to have a function that solely exists in another function (ie super_list_maker here)\n",
    "\n",
    "def embedding_returner(list_of_lists = True, minC = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function utilizes the above function, as well as the imported Louisa w2v functions to return a list of length 500 word embeddings. Using the above \n",
    "    function it will prompt the user for a local corpus location as well as for file types and the name of the model to be saved. Currently it only reads \n",
    "    in .json and .txt files. It will also print out the unique tokens generated from BERT tokenization for each book. The default variable is passed into\n",
    "    the super_list_maker function, and the minC default variable is the minimum count variable passed into Louisa's w2v_train function for word2vec. The \n",
    "    function returns a dictionary with keys: \"embeddings\" and \"words/tokens\".\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_tokens = super_list_maker(separate = list_of_lists)\n",
    "    if list_of_tokens:\n",
    "        saveName = input(\"Enter a name for the model saved: \")\n",
    "        w2v_functions.tokens = list_of_tokens\n",
    "        vec_emb_dict = w2v_functions.w2v_train(saveName, min_count = minC)               #issue here changing it to dictionary, bc this fxn doesn't return\n",
    "        print(\"Number of unique word embeddings generated: \", len(vec_emb_dict[\"embeddings\"]))\n",
    "        print(\"Size of word embeddings: \", len(vec_emb_dict[\"embeddings\"][0]))\n",
    "        return vec_emb_dict\n",
    "    else:\n",
    "        print(\"Check your corpus location and your data types!\")\n",
    "\n",
    "vecs_and_words_dict = dict()\n",
    "vecs_and_words_dict = embedding_returner()\n",
    "json = json.dumps(vecs_and_words_dict)\n",
    "\n",
    "SaveName = input(\"Enter a file name for the .json file of words and embeddings\")\n",
    "SaveName = SaveName + \".json\"\n",
    "\n",
    "f = open(SaveName,\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This function/part works just needs to have the kernal reset each time for some reason?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This function/part works just needs to have the kernal reset each time for some reason?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

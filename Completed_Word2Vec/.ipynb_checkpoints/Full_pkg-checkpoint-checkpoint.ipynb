{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 11:29:08: built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The purpose of this notebook is to write a .py callable program which takes in a local library location and returns a list of BERT tokenized word2vec vectors\n",
    "\"\"\"\n",
    "\n",
    "#need to double check if we need these if they're already in Lousia's functions\n",
    "import re, string \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, PreTrainedTokenizer\n",
    "import csv\n",
    "import glob \n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "#louisa's functions imported from local download as .py, added lines = true to fix value error when reading json into df\n",
    "import Louisa_w2v_functions as w2v_functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def super_list_maker(separate = True):\n",
    "    \n",
    "    #This function prompts the user for a local folder/corpus location where the .json and .txt files are located. It then reads all of them into a list or \n",
    "    #list of lists depending on the default variable separate(true => list of lists vs false => single token list). In either case, the final list\n",
    "    #is then returned. For users on windows, the filepath format leading to the corpus would be: C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files for\n",
    "    #example\n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    initPath = input(\"Enter a file location\")\n",
    "    bookType = input(\"Enter a data type(.json please)\")\n",
    "    token_list = []\n",
    "    final_list = []\n",
    "    sep_token_list_of_lists = []\n",
    "    if bookType:\n",
    "        path = initPath + \"\\*\" + bookType\n",
    "    else:\n",
    "        path = initPath\n",
    "    raw_path = r\"{}\".format(path)\n",
    "    bookList = glob.glob(raw_path)\n",
    "    \n",
    "    yesOrNo = input(\"Is there an additional file type? (y or n) \")\n",
    "    if yesOrNo == \"y\":\n",
    "        bookType2 = input(\"Enter a second data type(.txt) \")\n",
    "        path2 = initPath + \"\\*\" + bookType2\n",
    "        raw_path2 = r\"{}\".format(path2)\n",
    "        bookList2 = glob.glob(raw_path2)\n",
    "        for bookSite in bookList2:\n",
    "            bookList.append(bookSite)\n",
    "    \n",
    "    bookCount = len(bookList)\n",
    "    print(\"bookCount is \", bookCount)\n",
    "    print(\"book list is \", bookList)\n",
    "\n",
    "    \n",
    "    for i in range(bookCount): #(len(bookList)):\n",
    "        sep_book_list = w2v_functions.feed2vec(bookList[i], tokenize = True)\n",
    "        sep_token_list_of_lists.append(sep_book_list[0])\n",
    "        print(\"token list of book \", i+1, \"is \", sep_book_list[0])\n",
    "        print(\"tokens in book \", i+1, \"is\", len(sep_book_list[0]))\n",
    "        print()\n",
    "        for word in sep_book_list[0]:\n",
    "            token_list.append(word)  #use true here\n",
    "        print(\"data type of token_list[\", i+1, \"]\", type(token_list[i]))\n",
    "\n",
    "    print()\n",
    "    print(\"length of vocab \", len(token_list))\n",
    "    \n",
    "    if separate == False:\n",
    "        print(\"final list length/token count \", len(token_list))\n",
    "        print (\"all tokens \", token_list)\n",
    "        return token_list\n",
    "    else:\n",
    "        print(\"list of all tokens (list of list form) \", sep_token_list_of_lists)\n",
    "        return sep_token_list_of_lists\n",
    "    print()\n",
    "\n",
    "\n",
    "#local book path = C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files   this format is what works, then use .json as type  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a file location C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\n",
      "Enter a data type(.json please) .json\n",
      "Is there an additional file type? (y or n)  y\n",
      "Enter a second data type(.txt)  .txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookCount is  7\n",
      "book list is  ['C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Polymer_Chemistry_by_Koltzsenburg_Maskos_and_Nuyken.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Polymer_Synthesis_Theory_and_Practice_by_Braun_Cherdron_Rehahn_Ritter_and_Voit.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Book Basic Principles of Organic Chemistry (Roberts and Caserio).txt', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Book Logic of Organic Synthesis (Rao).txt', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Book Organic Chemistry - A Carbonyl Early Approach (McMichael).txt', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Soderberg_bio_o_chem.txt']\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json\n",
      "                                                   0\n",
      "0  michael mosher · kenneth trantham  brewing  sc...\n",
      "1  all rights are reserved by the publisher, whet...\n",
      "2  the use of general descriptive names, register...\n",
      "3  the publisher, the authors and the editors are...\n",
      "4  neither the publisher nor the authors or the e...\n",
      "5                     trademarks, service marks, etc\n",
      "6   printed on acid-free paper  this springer imp...\n",
      "7  thus began the discussion for the beginnings o...\n",
      "8  “wouldn’t it be awesome,” we thought, “if we c...\n",
      "9              the result is what you will find here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:29: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:29:29: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list of book  1 is  ['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '4', 'neither', 'publisher', 'authors', 'editors', 'give', 'warrant', '.', '.', '.', '.', '.', '.', '131', '##7', '##6', '131', '##7', '##7', 'k', 'g', 'k', 'j', 'k', 'p', 'r', 'n', 'e', 'g', 'k', 'j', 'k', 'p', 'l', 'h', 'n', 'e', 'g', 'k', 'j', 'k', 'g', '.', '.', '.', '131', '##7', '##8', 'b', 'e', 'l', 'b', 'appendix', 'b', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '.', '.', '.', '131', '##7', '##9', 'moshe', '##r', 'k', '131', '##80', 'tran', '##tham', 'brewing', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '81', '##7', '##9', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  1 is 146\n",
      "\n",
      "data type of token_list[ 1 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Polymer_Chemistry_by_Koltzsenburg_Maskos_and_Nuyken.json\n",
      "                                                   0\n",
      "0  sebastian koltzenburg michael maskos oskar nuy...\n",
      "1  all rights are reserved by the publisher, whet...\n",
      "2  the  use  of  general  descriptive  names,  re...\n",
      "3   in  this  publication does not imply, even in...\n",
      "4  the publisher, the authors and the editors are...\n",
      "5   neither  the  publisher  nor  the  authors or...\n",
      "6   cover illustration: with kind permission by g...\n",
      "7   all computer chips used in our desktops, lapt...\n",
      "8   cutting-edge  biomedical  applications  requi...\n",
      "9  the interior of every automobile is almost ent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:30: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:29:31: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list of book  2 is  ['0', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'michael', 'mask', '##os', 'oskar', 'nu', '##y', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '.', '.', '.', '4', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '.', '.', '.', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '.', '.', '.', '950', '##5', 'gut', '##ta', 'perch', '##a', 'h', 'ha', '##f', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'h', '.', '.', '.', '950', '##6', 'see', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', 'pe', '##llet', '##izing', 'cold', 'pe', '##ll', '.', '.', '.', '950', '##7', 'see', 'forming', 'processes', 'resin', 'transfer', 'mold', '##ing', 'r', '.', '.', '.', '950', '##8', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', '##er', 'transit', '##io', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '86', '##8', '##2', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  2 is 162\n",
      "\n",
      "data type of token_list[ 2 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Polymer_Synthesis_Theory_and_Practice_by_Braun_Cherdron_Rehahn_Ritter_and_Voit.json\n",
      "                                                   0\n",
      "0          polymer synthesis: theory and practice   \n",
      "1    dietrich braun (29) harald cherdron matthias...\n",
      "2  dresden germany  isbn 978-3-642-28979-8 doi 10...\n",
      "3  all rights are reserved by the publisher, whet...\n",
      "4  exempted from this legal reservation are brief...\n",
      "5  duplication of this publication or parts there...\n",
      "6  permissions for use may be obtained through ri...\n",
      "7  violations are liable to prosecution under the...\n",
      "8  the use of general descriptive names, register...\n",
      "9  in this publication does not imply, even in th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:32: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:29:32: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list of book  3 is  ['0', 'polymer', 'synthesis', 'theory', 'practice', '1', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', 're', '##ha', '##hn', '.', '.', '.', '2', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'n', '.', '.', '.', '3', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '4', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '.', '.', '.', '.', '.', '.', '122', '##60', 'braun', 'et', 'al', 'polymer', 'synthesis', 'theory', 'practice', '.', '.', '.', '122', '##6', '##1', 'see', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', '##r', 'of', '##et', 'ole', '##d', '122', '##6', '##2', 'see', 'organic', 'light', 'emi', '##tting', 'di', '##ode', 'ole', '##d', 'ol', '##igo', '##mer', '.', '.', '.', '122', '##6', '##3', 'see', 'organic', 'photo', 'voltage', 'op', '##v', 'organic', 'field', '##ef', '##f', '.', '.', '.', '122', '##64', 'see', 'polymer', 'based', 'organic', 'light', 'emi', '##tting', 'di', '##ode', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '70', '##57', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  3 is 143\n",
      "\n",
      "data type of token_list[ 3 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Book Basic Principles of Organic Chemistry (Roberts and Caserio).txt\n",
      "                                                   0\n",
      "0  1: Introduction to Organic Chemistry: You now ...\n",
      "1  2: Structural Organic Chemistry: This chapter ...\n",
      "2  3: Organic Nomenclature: A chemical nomenclatu...\n",
      "3  4: Alkanes: Alkanes are the simplest organic m...\n",
      "4  5: Stereoisomerism of Organic Molecules: Posit...\n",
      "5  6: Bonding in Organic Molecules: Remembering t...\n",
      "6  7: Other Compounds than Hydrocarbons: We begin...\n",
      "7  8: Nucleophilic Substitution and Elimination R...\n",
      "8  9: Separation, Purification, & Identification ...\n",
      "9  10: Alkenes and Alkynes I - Ionic and Radical ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:35: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:29:35: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list of book  4 is  ['0', 'introduction', 'organic', 'chemistry', 'starting', 'study', '.', '.', '.', '1', 'structural', 'organic', 'chemistry', 'chapter', 'briefly', 'r', '.', '.', '.', '2', 'organic', 'nomenclature', 'chemical', 'nomenclature', 'set', '.', '.', '.', '3', 'al', '##kan', '##es', 'al', '##kan', '##es', 'simplest', 'organic', 'molecules', 'con', '.', '.', '.', '4', 'stereo', '##iso', '##mer', '##ism', 'organic', 'molecules', 'position', 'iso', '.', '.', '.', '.', '.', '.', '71', '##50', 'show', 'mechanisms', 'combination', 'others', 'described', 'c', '.', '.', '.', '71', '##51', 'ch', 'ch', 'pd', '##cl', 'h', 'ch', '71', '##52', 'reaction', 'used', 'large', '##sca', '##le', 'production', 'ox', '##idi', '##zing', '.', '.', '.', '71', '##53', 'b', 'balance', 'competitive', 'nu', '##cle', '##op', '##hil', '##ic', 'reactions', 'd', '.', '.', '.', '71', '##54', 'write', 'me', '##chan', '##istic', 'steps', 'account', 'difference', 'ste', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '67', '##42', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  4 is 131\n",
      "\n",
      "data type of token_list[ 4 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Book Logic of Organic Synthesis (Rao).txt\n",
      "                                                   0\n",
      "0                  1: Synthesis of Organic Molecules\n",
      "1  2: Rules and Guidelines Governing Organic Synt...\n",
      "2          Baldwin’s Rule for Ring Closure Reactions\n",
      "3                                       Bredt's Rule\n",
      "4                      Cram's Rule and Prelog's Rule\n",
      "5                  Hofmann’s Rule and Zaitsev’s Rule\n",
      "6                                   Markovnikov Rule\n",
      "7   3: Criteria for Selection of the Synthetic Route\n",
      "8                          4: The Logic of Synthesis\n",
      "9              5: Strategies in Disparlure Synthesis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:35: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:29:36: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list of book  5 is  ['0', 'synthesis', 'organic', 'molecules', '1', 'rules', 'guidelines', 'governing', 'organic', 'synthesis', '2', 'baldwin', '’', 's', 'rule', 'ring', 'closure', 'reactions', '3', 'bred', '##ts', 'rule', '4', 'cr', '##ams', 'rule', 'pre', '##log', '##s', 'rule', '.', '.', '.', '580', 'fig', '58', '##1', 'use', 'transition', 'metal', 're', '##age', '##nts', 'could', 'aid', 'con', '##st', '##r', '.', '.', '.', '58', '##2', 'fig', '58', '##3', 'rare', 'molecular', 'engineering', 'marvel', '##s', 'like', 'rb', 'woo', '.', '.', '.', '58', '##4', 'doubt', 'feasibility', 'protection', 'free', 'synthesis', 'co', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '49', '##9', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  5 is 91\n",
      "\n",
      "data type of token_list[ 5 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Book Organic Chemistry - A Carbonyl Early Approach (McMichael).txt\n",
      "                                                   0\n",
      "0    1: Carbonyl Group: Notation, Structure, Bonding\n",
      "1        2: Functional Groups, Hybridization, Naming\n",
      "2       3: Additions: Electrophilic and Nucleophilic\n",
      "3          4: Acetal Formation, Mechanism, Resonance\n",
      "4         5: Nitrogen Nucleophiles - Imine Formation\n",
      "5          6: Addition of Organometallics - Grignard\n",
      "6        7: Oxidation & Reduction, alpha-C-H acidity\n",
      "7         8: Enolates, Aldol Condensation, Synthesis\n",
      "8    9: Carboxylic Acid Derivatives: Interconversion\n",
      "9  10: Carboxylic Acid Derivatives - Alpha Carbon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:37: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:29:37: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list of book  6 is  ['0', 'carbon', '##yl', 'group', 'notation', 'structure', 'bonding', '1', 'functional', 'groups', 'hybrid', '##ization', 'naming', '2', 'additions', 'electro', '##phi', '##lic', 'nu', '##cle', '##op', '##hil', '##ic', '3', 'ace', '##tal', 'formation', 'mechanism', 'resonance', '4', 'nitrogen', 'nu', '##cle', '##op', '##hil', '##es', 'im', '##ine', 'formation', '.', '.', '.', '101', '##8', 'finally', 'lets', 'return', 'another', 'application', 'free', 'r', '.', '.', '.', '102', '##0', 'new', 'molecule', 'would', 'also', 'free', 'radical', 'could', 'add', '.', '.', '.', '102', '##1', 'polymer', '##ization', 'reactions', 'also', 'important', 'table', '.', '.', '.', '102', '##3', 'since', 'repeating', 'unit', 'formed', 'adding', 'end', 'double', '.', '.', '.', '102', '##4', 'kirk', 'mc', '##mic', '##hae', '##l', 'washington', 'state', 'university', 'name', ':', 'text', ',', 'length', ':', '84', '##8', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  6 is 113\n",
      "\n",
      "data type of token_list[ 6 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Soderberg_bio_o_chem.txt\n",
      "                                                   0\n",
      "0  1: Introduction to Organic Structure and Bondi...\n",
      "1  2: Introduction to Organic Structure and Bondi...\n",
      "2               3: Conformations and Stereochemistry\n",
      "3  4: Structure Determination I- UV-Vis and Infra...\n",
      "4  5: Structure Determination Part II - Nuclear M...\n",
      "5                  6: Overview of Organic Reactivity\n",
      "6                             7: Acid-base Reactions\n",
      "7             8: Nucleophilic Substitution Reactions\n",
      "8  9: Phosphate Transfer Reactions: This chapter ...\n",
      "9       10: Nucleophilic Carbonyl Addition Reactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:39: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:29:39: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list of book  7 is  ['0', 'introduction', 'organic', 'structure', 'bonding', 'chapter', '.', '.', '.', '1', 'introduction', 'organic', 'structure', 'bonding', 'ii', '2', 'conform', '##ations', 'stereo', '##chemist', '##ry', '3', 'structure', 'determination', 'uv', '##vis', 'infrared', 'spec', '##tro', '.', '.', '.', '4', 'structure', 'determination', 'part', 'ii', 'nuclear', 'magnet', '.', '.', '.', '.', '.', '.', '425', '##6', 'phase', '425', '##7', 'phase', '425', '##8', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', '425', '##9', 'completing', 'chapter', 'able', '42', '##60', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '40', '##8', '##1', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  7 is 94\n",
      "\n",
      "data type of token_list[ 7 ] <class 'str'>\n",
      "\n",
      "length of vocab  880\n",
      "list of all tokens (list of list form)  [['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '4', 'neither', 'publisher', 'authors', 'editors', 'give', 'warrant', '.', '.', '.', '.', '.', '.', '131', '##7', '##6', '131', '##7', '##7', 'k', 'g', 'k', 'j', 'k', 'p', 'r', 'n', 'e', 'g', 'k', 'j', 'k', 'p', 'l', 'h', 'n', 'e', 'g', 'k', 'j', 'k', 'g', '.', '.', '.', '131', '##7', '##8', 'b', 'e', 'l', 'b', 'appendix', 'b', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '.', '.', '.', '131', '##7', '##9', 'moshe', '##r', 'k', '131', '##80', 'tran', '##tham', 'brewing', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '81', '##7', '##9', ',', 'dt', '##ype', ':', 'object'], ['0', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'michael', 'mask', '##os', 'oskar', 'nu', '##y', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '.', '.', '.', '4', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '.', '.', '.', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '.', '.', '.', '950', '##5', 'gut', '##ta', 'perch', '##a', 'h', 'ha', '##f', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'h', '.', '.', '.', '950', '##6', 'see', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', 'pe', '##llet', '##izing', 'cold', 'pe', '##ll', '.', '.', '.', '950', '##7', 'see', 'forming', 'processes', 'resin', 'transfer', 'mold', '##ing', 'r', '.', '.', '.', '950', '##8', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', '##er', 'transit', '##io', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '86', '##8', '##2', ',', 'dt', '##ype', ':', 'object'], ['0', 'polymer', 'synthesis', 'theory', 'practice', '1', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', 're', '##ha', '##hn', '.', '.', '.', '2', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'n', '.', '.', '.', '3', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '4', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '.', '.', '.', '.', '.', '.', '122', '##60', 'braun', 'et', 'al', 'polymer', 'synthesis', 'theory', 'practice', '.', '.', '.', '122', '##6', '##1', 'see', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', '##r', 'of', '##et', 'ole', '##d', '122', '##6', '##2', 'see', 'organic', 'light', 'emi', '##tting', 'di', '##ode', 'ole', '##d', 'ol', '##igo', '##mer', '.', '.', '.', '122', '##6', '##3', 'see', 'organic', 'photo', 'voltage', 'op', '##v', 'organic', 'field', '##ef', '##f', '.', '.', '.', '122', '##64', 'see', 'polymer', 'based', 'organic', 'light', 'emi', '##tting', 'di', '##ode', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '70', '##57', ',', 'dt', '##ype', ':', 'object'], ['0', 'introduction', 'organic', 'chemistry', 'starting', 'study', '.', '.', '.', '1', 'structural', 'organic', 'chemistry', 'chapter', 'briefly', 'r', '.', '.', '.', '2', 'organic', 'nomenclature', 'chemical', 'nomenclature', 'set', '.', '.', '.', '3', 'al', '##kan', '##es', 'al', '##kan', '##es', 'simplest', 'organic', 'molecules', 'con', '.', '.', '.', '4', 'stereo', '##iso', '##mer', '##ism', 'organic', 'molecules', 'position', 'iso', '.', '.', '.', '.', '.', '.', '71', '##50', 'show', 'mechanisms', 'combination', 'others', 'described', 'c', '.', '.', '.', '71', '##51', 'ch', 'ch', 'pd', '##cl', 'h', 'ch', '71', '##52', 'reaction', 'used', 'large', '##sca', '##le', 'production', 'ox', '##idi', '##zing', '.', '.', '.', '71', '##53', 'b', 'balance', 'competitive', 'nu', '##cle', '##op', '##hil', '##ic', 'reactions', 'd', '.', '.', '.', '71', '##54', 'write', 'me', '##chan', '##istic', 'steps', 'account', 'difference', 'ste', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '67', '##42', ',', 'dt', '##ype', ':', 'object'], ['0', 'synthesis', 'organic', 'molecules', '1', 'rules', 'guidelines', 'governing', 'organic', 'synthesis', '2', 'baldwin', '’', 's', 'rule', 'ring', 'closure', 'reactions', '3', 'bred', '##ts', 'rule', '4', 'cr', '##ams', 'rule', 'pre', '##log', '##s', 'rule', '.', '.', '.', '580', 'fig', '58', '##1', 'use', 'transition', 'metal', 're', '##age', '##nts', 'could', 'aid', 'con', '##st', '##r', '.', '.', '.', '58', '##2', 'fig', '58', '##3', 'rare', 'molecular', 'engineering', 'marvel', '##s', 'like', 'rb', 'woo', '.', '.', '.', '58', '##4', 'doubt', 'feasibility', 'protection', 'free', 'synthesis', 'co', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '49', '##9', ',', 'dt', '##ype', ':', 'object'], ['0', 'carbon', '##yl', 'group', 'notation', 'structure', 'bonding', '1', 'functional', 'groups', 'hybrid', '##ization', 'naming', '2', 'additions', 'electro', '##phi', '##lic', 'nu', '##cle', '##op', '##hil', '##ic', '3', 'ace', '##tal', 'formation', 'mechanism', 'resonance', '4', 'nitrogen', 'nu', '##cle', '##op', '##hil', '##es', 'im', '##ine', 'formation', '.', '.', '.', '101', '##8', 'finally', 'lets', 'return', 'another', 'application', 'free', 'r', '.', '.', '.', '102', '##0', 'new', 'molecule', 'would', 'also', 'free', 'radical', 'could', 'add', '.', '.', '.', '102', '##1', 'polymer', '##ization', 'reactions', 'also', 'important', 'table', '.', '.', '.', '102', '##3', 'since', 'repeating', 'unit', 'formed', 'adding', 'end', 'double', '.', '.', '.', '102', '##4', 'kirk', 'mc', '##mic', '##hae', '##l', 'washington', 'state', 'university', 'name', ':', 'text', ',', 'length', ':', '84', '##8', ',', 'dt', '##ype', ':', 'object'], ['0', 'introduction', 'organic', 'structure', 'bonding', 'chapter', '.', '.', '.', '1', 'introduction', 'organic', 'structure', 'bonding', 'ii', '2', 'conform', '##ations', 'stereo', '##chemist', '##ry', '3', 'structure', 'determination', 'uv', '##vis', 'infrared', 'spec', '##tro', '.', '.', '.', '4', 'structure', 'determination', 'part', 'ii', 'nuclear', 'magnet', '.', '.', '.', '.', '.', '.', '425', '##6', 'phase', '425', '##7', 'phase', '425', '##8', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', '425', '##9', 'completing', 'chapter', 'able', '42', '##60', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '40', '##8', '##1', ',', 'dt', '##ype', ':', 'object']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a name for the model saved:  fourBook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:45: collecting all words and their counts\n",
      "INFO - 11:29:45: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 11:29:45: collected 382 word types from a corpus of 880 raw words and 7 sentences\n",
      "INFO - 11:29:45: Loading a fresh vocabulary\n",
      "INFO - 11:29:45: effective_min_count=1 retains 382 unique words (100% of original 382, drops 0)\n",
      "INFO - 11:29:45: effective_min_count=1 leaves 880 word corpus (100% of original 880, drops 0)\n",
      "INFO - 11:29:45: deleting the raw counts dictionary of 382 items\n",
      "INFO - 11:29:45: sample=0.001 downsamples 74 most-common words\n",
      "INFO - 11:29:45: downsampling leaves estimated 584 word corpus (66.5% of prior 880)\n",
      "INFO - 11:29:45: estimated required memory for 382 words and 500 dimensions: 1719000 bytes\n",
      "INFO - 11:29:45: resetting layer weights\n",
      "INFO - 11:29:45: training model with 3 workers on 382 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 1 : training on 880 raw words (580 effective words) took 0.0s, 69611 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 2 : training on 880 raw words (580 effective words) took 0.0s, 63783 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 3 : training on 880 raw words (587 effective words) took 0.0s, 76822 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 4 : training on 880 raw words (576 effective words) took 0.0s, 74618 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 5 : training on 880 raw words (598 effective words) took 0.0s, 74749 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 6 : training on 880 raw words (594 effective words) took 0.0s, 69449 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 7 : training on 880 raw words (592 effective words) took 0.0s, 71873 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 8 : training on 880 raw words (584 effective words) took 0.0s, 72454 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 9 : training on 880 raw words (586 effective words) took 0.0s, 82428 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 10 : training on 880 raw words (597 effective words) took 0.0s, 83326 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 11 : training on 880 raw words (574 effective words) took 0.0s, 65988 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 12 : training on 880 raw words (575 effective words) took 0.0s, 75377 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 13 : training on 880 raw words (583 effective words) took 0.0s, 74463 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 14 : training on 880 raw words (587 effective words) took 0.0s, 76189 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:45: EPOCH - 15 : training on 880 raw words (590 effective words) took 0.0s, 82553 effective words/s\n",
      "INFO - 11:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 16 : training on 880 raw words (593 effective words) took 0.0s, 71313 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 17 : training on 880 raw words (590 effective words) took 0.0s, 63552 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 18 : training on 880 raw words (577 effective words) took 0.0s, 61361 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 19 : training on 880 raw words (585 effective words) took 0.0s, 82215 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 20 : training on 880 raw words (588 effective words) took 0.0s, 75481 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 21 : training on 880 raw words (574 effective words) took 0.0s, 72952 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 22 : training on 880 raw words (577 effective words) took 0.0s, 62817 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 23 : training on 880 raw words (585 effective words) took 0.0s, 75970 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 24 : training on 880 raw words (578 effective words) took 0.0s, 71458 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 25 : training on 880 raw words (590 effective words) took 0.0s, 80323 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 26 : training on 880 raw words (587 effective words) took 0.0s, 75865 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 27 : training on 880 raw words (583 effective words) took 0.0s, 74379 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 28 : training on 880 raw words (598 effective words) took 0.0s, 99785 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 29 : training on 880 raw words (600 effective words) took 0.0s, 82388 effective words/s\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:29:46: EPOCH - 30 : training on 880 raw words (592 effective words) took 0.0s, 79456 effective words/s\n",
      "INFO - 11:29:46: training on a 26400 raw words (17580 effective words) took 0.4s, 42200 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.01 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:46: saving Word2VecKeyedVectors object under C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv, separately None\n",
      "INFO - 11:29:46: not storing attribute vectors_norm\n",
      "INFO - 11:29:46: saved C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv\n",
      "INFO - 11:29:46: loading Word2VecKeyedVectors object from C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv\n",
      "INFO - 11:29:46: setting ignored attribute vectors_norm to None\n",
      "INFO - 11:29:46: loaded C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus count is  7\n",
      "epochs is  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:46: saving Word2Vec object under fourBook, separately None\n",
      "INFO - 11:29:46: not storing attribute vectors_norm\n",
      "INFO - 11:29:46: not storing attribute cum_table\n",
      "INFO - 11:29:46: saved fourBook\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyed vectors  <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x000002233022E648>\n",
      "all words in wv.vocab are  ['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '2', 'use', 'general', 'descriptive', 'names', 'registered', '3', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '4', 'neither', 'give', 'warrant', '131', '##7', '##6', 'k', 'g', 'j', 'p', 'r', 'n', 'e', 'l', 'h', '##8', 'b', 'appendix', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '##9', '##80', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', 'name', ':', 'text', ',', 'length', '81', 'dt', '##ype', 'object', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'mask', '##os', 'oskar', 'nu', '##y', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '##5', 'gut', '##ta', 'perch', '##a', 'ha', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', '##llet', '##izing', 'cold', '##ll', 'forming', 'processes', 'resin', 'transfer', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', 'transit', '##io', '86', '##2', 'polymer', 'synthesis', 'theory', 'practice', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', '##ha', '##hn', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '122', '##60', 'et', 'al', '##1', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', 'of', '##et', 'ole', 'light', 'emi', '##tting', 'di', '##ode', 'ol', '##igo', '##mer', '##3', 'photo', 'voltage', 'op', '##v', '##64', 'based', '70', '##57', 'introduction', 'chemistry', 'starting', 'study', 'structural', 'chapter', 'briefly', 'nomenclature', 'chemical', 'set', '##kan', '##es', 'simplest', 'molecules', 'stereo', '##iso', '##ism', 'position', 'iso', '71', '##50', 'show', 'mechanisms', 'combination', 'others', 'described', 'c', '##51', 'ch', 'pd', '##cl', '##52', 'reaction', 'used', 'large', '##sca', '##le', 'production', 'ox', '##idi', '##zing', '##53', 'balance', 'competitive', '##cle', '##op', '##hil', '##ic', 'reactions', 'd', '##54', 'write', 'me', '##chan', '##istic', 'steps', 'account', 'difference', 'ste', '67', '##42', 'rules', 'guidelines', 'governing', 'baldwin', 'rule', 'ring', 'closure', 'bred', '##ts', 'cr', '##ams', 'pre', '##log', '##s', '580', 'fig', '58', 'transition', 'metal', '##nts', 'could', 'aid', '##st', 'rare', 'molecular', 'engineering', 'marvel', 'like', 'rb', 'woo', 'doubt', 'feasibility', 'protection', 'free', 'co', '49', 'carbon', '##yl', 'group', 'notation', 'structure', 'bonding', 'functional', 'groups', 'hybrid', '##ization', 'naming', 'additions', 'electro', '##phi', '##lic', 'ace', '##tal', 'formation', 'mechanism', 'resonance', 'nitrogen', 'im', '##ine', '101', 'finally', 'lets', 'return', 'another', 'application', '102', '##0', 'new', 'molecule', 'would', 'also', 'radical', 'add', 'important', 'since', 'repeating', 'unit', 'formed', 'adding', 'end', 'double', 'kirk', 'mc', '##mic', '##hae', '##l', 'washington', 'university', '84', 'ii', 'conform', '##ations', '##chemist', '##ry', 'determination', 'uv', '##vis', 'infrared', 'spec', '##tro', 'nuclear', 'magnet', '425', 'phase', 'biological', 'emphasis', 'tim', 'so', '##de', 'completing', 'able', '42', '40']\n",
      "number of words in wv.vocab is  382\n",
      "type of all vectors is:  <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vecs_and_words type:  <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "def embedding_returner(list_of_lists = True, miniC = 1):\n",
    "    \n",
    "    #This function utilizes the above function, as well as the imported Louisa w2v functions to return a list of length 500 word embeddings. It will prompt the \n",
    "    #user for a local corpus location (for windows the format is C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files), as well as for file types. Currently it \n",
    "    #only reads in .json and .txt files. It will also print out the unique tokens generated from BERT tokenization as well as from the embeddings produced from\n",
    "    #word2vec\n",
    "    \n",
    "    \n",
    "    list_of_tokens = super_list_maker(separate = list_of_lists)\n",
    "    if list_of_tokens:\n",
    "        saveName = input(\"Enter a name for the model saved: \")\n",
    "        w2v_functions.tokens = list_of_tokens\n",
    "        vec_emb_dict = w2v_functions.w2v_train(saveName, min_count = miniC)               #issue here changing it to dictionary, bc this fxn doesn't return\n",
    "        print(\"Number of unique word embeddings generated: \", len(vec_emb_dict[\"embeddings\"]))\n",
    "        print(\"Size of word embeddings: \", len(vec_emb_dict[\"embeddings\"][0]))\n",
    "        return vec_emb_dict\n",
    "    else:\n",
    "        print(\"Check your corpus location and your data types!\")\n",
    "\n",
    "vecs_and_words_dict = dict()\n",
    "vecs_and_words_dict = embedding_returner()\n",
    "json = json.dumps(vecs_and_words_dict)\n",
    "\n",
    "SaveName = input(\"Enter a file for the .json file of words and embeddings\")\n",
    "\n",
    "f = open(SaveName,\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This function/part can handle the TSNE graphing, display to user and such'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

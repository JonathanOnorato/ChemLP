{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The purpose of this notebook is to write a .py callable program which does five things: 1) create a word2vec neural network 2) optimize the parameters for\n",
    "NN creation 3) save the NN 4) graph the data, and 5) make the word embeddings compatible with the embeddings created by BERT. Not neccessarily in that order.\n",
    "It should take in the locationof a data corpus, the desired name of the saved NN\"\"\"\n",
    "\n",
    "#need to double check if we need these if they're already in Lousia's functions\n",
    "import re, string \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, PreTrainedTokenizer\n",
    "import csv\n",
    "import glob \n",
    "\n",
    "#louisa's functions imported from local download as .py, added lines = true to fix value error when reading json into df\n",
    "import Louisa_w2v_functions as w2v_functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a quick function that prompts the user for a local corpus folder name then reads all the jsons in it, they then can be all cleaned w/ Louisa's clean\n",
    "#function or a one I wrote\n",
    "\n",
    "#w2v_functions.feed2vec(r\"C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json\", tokenize = True )\n",
    "\n",
    "def super_list_maker():\n",
    "    \"\"\"this function prompts the user for a local folder/corpus location, where all the json's are located. It then reads all of them into a list.\n",
    "    Currently it reads them into a list of lists, where each book is a list. Might be useful to have this distinction\"\"\"\n",
    "    initPath = input(\"Enter a file location\")\n",
    "    bookType = input(\"Enter a data type(.json please)\")\n",
    "    token_list = []\n",
    "    final_list = []\n",
    "    if bookType:\n",
    "        path = initPath + \"\\*\" + bookType\n",
    "    else:\n",
    "        path = initPath\n",
    "    raw_path = r\"{}\".format(path)\n",
    "    bookList = glob.glob(raw_path)\n",
    "    \n",
    "    yesOrNo = input(\"Is there an additional file type? (y or n) \")\n",
    "    if yesOrNo == \"y\":\n",
    "        bookType2 = input(\"Enter a second data type(.txt) \")\n",
    "        path2 = initPath + \"\\*\" + bookType2\n",
    "        raw_path2 = r\"{}\".format(path2)\n",
    "        bookList2 = glob.glob(raw_path2)\n",
    "        for bookSite in bookList2:\n",
    "            bookList.append(bookSite)\n",
    "    \n",
    "    bookCount = len(bookList)\n",
    "    print(\"bookCount is \", bookCount)\n",
    "    print(\"book list is \", bookList)\n",
    "\n",
    "    for i in range(bookCount - 1): #(len(bookList)):\n",
    "        sep_book_list = w2v_functions.feed2vec(bookList[i], tokenize = True)\n",
    "        print(\"token list as of book \", i+1, \"is \", sep_book_list)\n",
    "        print(\"words in book \", i+1, \"is\", len(sep_book_list[0]))\n",
    "        print()\n",
    "        for word in sep_book_list:\n",
    "            token_list.append(word)  #use true here\n",
    "        print(\"data type of token_list[\", i, \"]\", type(token_list[i]))\n",
    "        if type(token_list[i]) == list:\n",
    "            for subtoken in token_list[i]:\n",
    "                final_list.append(subtoken)\n",
    "\n",
    "    print()\n",
    "    print(\"number of books \", len(token_list) + 1)\n",
    "    print(\"final list length/token count \", len(final_list))\n",
    "    print (\"all tokens \", final_list)\n",
    "    return final_list\n",
    "    print()\n",
    "\n",
    "    \n",
    "#local book path = C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files   this format is what works, then use .json as type  \n",
    "#super_list_maker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a file location C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\n",
      "Enter a data type(.json please) .json\n",
      "Is there an additional file type? (y or n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookCount is  4\n",
      "book list is  ['C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Polymer_Chemistry_by_Koltzsenburg_Maskos_and_Nuyken.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Polymer_Synthesis_Theory_and_Practice_by_Braun_Cherdron_Rehahn_Ritter_and_Voit.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Principles_of_Polymer_Chemistry_by_Ravve.json']\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json\n",
      "                                                   0\n",
      "0  michael mosher · kenneth trantham  brewing  sc...\n",
      "1  all rights are reserved by the publisher, whet...\n",
      "2  the use of general descriptive names, register...\n",
      "3  the publisher, the authors and the editors are...\n",
      "4  neither the publisher nor the authors or the e...\n",
      "5                     trademarks, service marks, etc\n",
      "6   printed on acid-free paper  this springer imp...\n",
      "7  thus began the discussion for the beginnings o...\n",
      "8  “wouldn’t it be awesome,” we thought, “if we c...\n",
      "9              the result is what you will find here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  1 is  [['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '4', 'neither', 'publisher', 'authors', 'editors', 'give', 'warrant', '.', '.', '.', '.', '.', '.', '131', '##7', '##6', '131', '##7', '##7', 'k', 'g', 'k', 'j', 'k', 'p', 'r', 'n', 'e', 'g', 'k', 'j', 'k', 'p', 'l', 'h', 'n', 'e', 'g', 'k', 'j', 'k', 'g', '.', '.', '.', '131', '##7', '##8', 'b', 'e', 'l', 'b', 'appendix', 'b', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '.', '.', '.', '131', '##7', '##9', 'moshe', '##r', 'k', '131', '##80', 'tran', '##tham', 'brewing', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '81', '##7', '##9', ',', 'dt', '##ype', ':', 'object']]\n",
      "words in book  1 is 146\n",
      "\n",
      "data type of token_list[ 0 ] <class 'list'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Polymer_Chemistry_by_Koltzsenburg_Maskos_and_Nuyken.json\n",
      "                                                   0\n",
      "0  sebastian koltzenburg michael maskos oskar nuy...\n",
      "1  all rights are reserved by the publisher, whet...\n",
      "2  the  use  of  general  descriptive  names,  re...\n",
      "3   in  this  publication does not imply, even in...\n",
      "4  the publisher, the authors and the editors are...\n",
      "5   neither  the  publisher  nor  the  authors or...\n",
      "6   cover illustration: with kind permission by g...\n",
      "7   all computer chips used in our desktops, lapt...\n",
      "8   cutting-edge  biomedical  applications  requi...\n",
      "9  the interior of every automobile is almost ent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  2 is  [['0', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'michael', 'mask', '##os', 'oskar', 'nu', '##y', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '.', '.', '.', '4', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '.', '.', '.', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '.', '.', '.', '950', '##5', 'gut', '##ta', 'perch', '##a', 'h', 'ha', '##f', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'h', '.', '.', '.', '950', '##6', 'see', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', 'pe', '##llet', '##izing', 'cold', 'pe', '##ll', '.', '.', '.', '950', '##7', 'see', 'forming', 'processes', 'resin', 'transfer', 'mold', '##ing', 'r', '.', '.', '.', '950', '##8', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', '##er', 'transit', '##io', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '86', '##8', '##2', ',', 'dt', '##ype', ':', 'object']]\n",
      "words in book  2 is 162\n",
      "\n",
      "data type of token_list[ 1 ] <class 'list'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Polymer_Synthesis_Theory_and_Practice_by_Braun_Cherdron_Rehahn_Ritter_and_Voit.json\n",
      "                                                   0\n",
      "0          polymer synthesis: theory and practice   \n",
      "1    dietrich braun (29) harald cherdron matthias...\n",
      "2  dresden germany  isbn 978-3-642-28979-8 doi 10...\n",
      "3  all rights are reserved by the publisher, whet...\n",
      "4  exempted from this legal reservation are brief...\n",
      "5  duplication of this publication or parts there...\n",
      "6  permissions for use may be obtained through ri...\n",
      "7  violations are liable to prosecution under the...\n",
      "8  the use of general descriptive names, register...\n",
      "9  in this publication does not imply, even in th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  3 is  [['0', 'polymer', 'synthesis', 'theory', 'practice', '1', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', 're', '##ha', '##hn', '.', '.', '.', '2', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'n', '.', '.', '.', '3', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '4', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '.', '.', '.', '.', '.', '.', '122', '##60', 'braun', 'et', 'al', 'polymer', 'synthesis', 'theory', 'practice', '.', '.', '.', '122', '##6', '##1', 'see', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', '##r', 'of', '##et', 'ole', '##d', '122', '##6', '##2', 'see', 'organic', 'light', 'emi', '##tting', 'di', '##ode', 'ole', '##d', 'ol', '##igo', '##mer', '.', '.', '.', '122', '##6', '##3', 'see', 'organic', 'photo', 'voltage', 'op', '##v', 'organic', 'field', '##ef', '##f', '.', '.', '.', '122', '##64', 'see', 'polymer', 'based', 'organic', 'light', 'emi', '##tting', 'di', '##ode', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '70', '##57', ',', 'dt', '##ype', ':', 'object']]\n",
      "words in book  3 is 143\n",
      "\n",
      "data type of token_list[ 2 ] <class 'list'>\n",
      "\n",
      "number of books  4\n",
      "final list length/token count  451\n",
      "all tokens  ['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '4', 'neither', 'publisher', 'authors', 'editors', 'give', 'warrant', '.', '.', '.', '.', '.', '.', '131', '##7', '##6', '131', '##7', '##7', 'k', 'g', 'k', 'j', 'k', 'p', 'r', 'n', 'e', 'g', 'k', 'j', 'k', 'p', 'l', 'h', 'n', 'e', 'g', 'k', 'j', 'k', 'g', '.', '.', '.', '131', '##7', '##8', 'b', 'e', 'l', 'b', 'appendix', 'b', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '.', '.', '.', '131', '##7', '##9', 'moshe', '##r', 'k', '131', '##80', 'tran', '##tham', 'brewing', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '81', '##7', '##9', ',', 'dt', '##ype', ':', 'object', '0', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'michael', 'mask', '##os', 'oskar', 'nu', '##y', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '.', '.', '.', '4', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '.', '.', '.', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '.', '.', '.', '950', '##5', 'gut', '##ta', 'perch', '##a', 'h', 'ha', '##f', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'h', '.', '.', '.', '950', '##6', 'see', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', 'pe', '##llet', '##izing', 'cold', 'pe', '##ll', '.', '.', '.', '950', '##7', 'see', 'forming', 'processes', 'resin', 'transfer', 'mold', '##ing', 'r', '.', '.', '.', '950', '##8', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', '##er', 'transit', '##io', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '86', '##8', '##2', ',', 'dt', '##ype', ':', 'object', '0', 'polymer', 'synthesis', 'theory', 'practice', '1', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', 're', '##ha', '##hn', '.', '.', '.', '2', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'n', '.', '.', '.', '3', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '4', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '.', '.', '.', '.', '.', '.', '122', '##60', 'braun', 'et', 'al', 'polymer', 'synthesis', 'theory', 'practice', '.', '.', '.', '122', '##6', '##1', 'see', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', '##r', 'of', '##et', 'ole', '##d', '122', '##6', '##2', 'see', 'organic', 'light', 'emi', '##tting', 'di', '##ode', 'ole', '##d', 'ol', '##igo', '##mer', '.', '.', '.', '122', '##6', '##3', 'see', 'organic', 'photo', 'voltage', 'op', '##v', 'organic', 'field', '##ef', '##f', '.', '.', '.', '122', '##64', 'see', 'polymer', 'based', 'organic', 'light', 'emi', '##tting', 'di', '##ode', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '70', '##57', ',', 'dt', '##ype', ':', 'object']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Not entirely sure what to do here; this function needs to read all of the local textbooks and build a word2vec vocab. The debate I'm having is whether\n",
    "or not it would be faster to train 1 model on a super-corpus of all the books, or keep updating the vocabulary w new books and training it each time. The first \n",
    "seems better to me. But it's going to produce a word2vec model w a built vocab that can then be trained iteratively until certain parameters are optimized\"\"\"\n",
    "\n",
    "#take in a vocab list\n",
    "\n",
    "list_of_tokens = super_list_maker()\n",
    "#saveName = input(\"Enter a name for the model saved: \")\n",
    "w2v_functions.tokens = list_of_tokens\n",
    "#w2v_functions.w2v_train(saveName)\n",
    "\n",
    "#1) find way to increase vocab size\n",
    "#2) use increase vocab size to then start optimzing w random sci kit learn search\n",
    "\n",
    "\n",
    "#def vocab_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alcohol or hydroxyl was not in the vocabulary\n",
      "ketone or carbonyl was not in the vocabulary\n",
      "alkene or alkyne was not in the vocabulary\n",
      "carbon or nitrogen was not in the vocabulary\n",
      "proton or hydrogen was not in the vocabulary\n",
      "polymer or chain was not in the vocabulary\n",
      "acid or base was not in the vocabulary\n",
      "oxidize or reduce was not in the vocabulary\n",
      "anion or cation was not in the vocabulary\n",
      "electrophile or nucleophile was not in the vocabulary\n",
      "polar or nonpolar was not in the vocabulary\n",
      "positive or negative was not in the vocabulary\n",
      "mechanism or atom was not in the vocabulary\n",
      "resonance or solvent was not in the vocabulary\n",
      "synthesis or electron was not in the vocabulary\n",
      "isomer or reaction was not in the vocabulary\n",
      "heat or bond was not in the vocabulary\n",
      "aromatic or equilibrium was not in the vocabulary\n",
      "carbon is not in the vocabulary\n",
      "Data collection and saving complete!\n",
      "['fourBook', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '4', 'neither', 'publisher', 'authors', 'editors', 'give', 'warrant', '.', '.', '.', '.', '.', '.', '131', '##7', '##6', '131', '##7', '##7', 'k', 'g', 'k', 'j', 'k', 'p', 'r', 'n', 'e', 'g', 'k', 'j', 'k', 'p', 'l', 'h', 'n', 'e', 'g', 'k', 'j', 'k', 'g', '.', '.', '.', '131', '##7', '##8', 'b', 'e', 'l', 'b', 'appendix', 'b', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '.', '.', '.', '131', '##7', '##9', 'moshe', '##r', 'k', '131', '##80', 'tran', '##tham', 'brewing', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '81', '##7', '##9', ',', 'dt', '##ype', ':', 'object', '0', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'michael', 'mask', '##os', 'oskar', 'nu', '##y', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '.', '.', '.', '4', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '.', '.', '.', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '.', '.', '.', '950', '##5', 'gut', '##ta', 'perch', '##a', 'h', 'ha', '##f', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'h', '.', '.', '.', '950', '##6', 'see', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', 'pe', '##llet', '##izing', 'cold', 'pe', '##ll', '.', '.', '.', '950', '##7', 'see', 'forming', 'processes', 'resin', 'transfer', 'mold', '##ing', 'r', '.', '.', '.', '950', '##8', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', '##er', 'transit', '##io', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '86', '##8', '##2', ',', 'dt', '##ype', ':', 'object', '0', 'polymer', 'synthesis', 'theory', 'practice', '1', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', 're', '##ha', '##hn', '.', '.', '.', '2', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'n', '.', '.', '.', '3', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '4', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '.', '.', '.', '.', '.', '.', '122', '##60', 'braun', 'et', 'al', 'polymer', 'synthesis', 'theory', 'practice', '.', '.', '.', '122', '##6', '##1', 'see', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', '##r', 'of', '##et', 'ole', '##d', '122', '##6', '##2', 'see', 'organic', 'light', 'emi', '##tting', 'di', '##ode', 'ole', '##d', 'ol', '##igo', '##mer', '.', '.', '.', '122', '##6', '##3', 'see', 'organic', 'photo', 'voltage', 'op', '##v', 'organic', 'field', '##ef', '##f', '.', '.', '.', '122', '##64', 'see', 'polymer', 'based', 'organic', 'light', 'emi', '##tting', 'di', '##ode', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '70', '##57', ',', 'dt', '##ype', ':', 'object']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This function is going to iteratively train the word2vec model until certain parameters are optimized. It's going to need grip search/sci-kit?\"\"\"\n",
    "\n",
    "#def trainer_optimizer\n",
    "\n",
    "first_word = ['alcohol', 'ketone', 'alkene', 'carbon', 'proton', 'polymer', 'acid', 'oxidize', 'anion', 'electrophile', 'polar', 'positive', 'mechanism', 'resonance', 'synthesis', 'isomer', 'heat', 'aromatic', ]\n",
    "second_word = ['hydroxyl', 'carbonyl', 'alkyne', 'nitrogen', 'hydrogen', 'chain', 'base', 'reduce', 'cation', 'nucleophile', 'nonpolar', 'negative', 'atom', 'solvent', 'electron', 'reaction', 'bond', 'equilibrium']\n",
    "\n",
    "w2v_functions.cosine_sim(\"fourBook\", first_word, second_word)\n",
    "print(w2v_functions.w2v_data)\n",
    "print(w2v_functions.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function/part can handle the TSNE graphing, display to user and such\"\"\"\n",
    "\n",
    "#def grapher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function will make the word2vec vectors compatible w/ the BERT vectors.\"\"\"\n",
    "\n",
    "#def vector_changer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main part of the program here. Call all the functions? Save the model/embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

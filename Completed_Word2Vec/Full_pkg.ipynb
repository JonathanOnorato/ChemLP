{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:32:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 14:32:43: built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The purpose of this notebook is to write a .py callable program which takes in a local library location and returns a list of BERT tokenized word2vec vectors\n",
    "\"\"\"\n",
    "\n",
    "#need to double check if we need these if they're already in Lousia's functions\n",
    "import re, string \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, PreTrainedTokenizer\n",
    "import csv\n",
    "import glob \n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "#louisa's functions imported from local download as .py, added lines = true to fix value error when reading json into df\n",
    "import Louisa_w2v_functions as w2v_functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def super_list_maker(separate = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function prompts the user for a local folder/corpus location where the .json and .txt files are located. It then reads all of them into a list or \n",
    "    list of lists depending on the default variable separate(true => list of lists vs false => single token list). In either case, the final list\n",
    "    is then returned. For users on windows, the filepath format leading to the corpus would be: C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files for\n",
    "    example.\n",
    "    \"\"\"\n",
    "    initPath = input(\"Enter a file location\")\n",
    "    bookType = input(\"Enter a data type(.json please)\")\n",
    "    token_list = []\n",
    "    final_list = []\n",
    "    sep_token_list_of_lists = []\n",
    "    if bookType:\n",
    "        path = initPath + \"\\*\" + bookType\n",
    "    else:\n",
    "        path = initPath\n",
    "    raw_path = r\"{}\".format(path)\n",
    "    bookList = glob.glob(raw_path)\n",
    "    \n",
    "    yesOrNo = input(\"Is there an additional file type? (y or n) \")\n",
    "    if yesOrNo == \"y\":\n",
    "        bookType2 = input(\"Enter a second data type(.txt) \")\n",
    "        path2 = initPath + \"\\*\" + bookType2\n",
    "        raw_path2 = r\"{}\".format(path2)\n",
    "        bookList2 = glob.glob(raw_path2)\n",
    "        for bookSite in bookList2:\n",
    "            bookList.append(bookSite)\n",
    "    \n",
    "    bookCount = len(bookList)\n",
    "    print(\"bookCount is \", bookCount)\n",
    "    print(\"book list is \", bookList)\n",
    "\n",
    "    \n",
    "    for i in range(bookCount): #(len(bookList)):\n",
    "        sep_book_list = w2v_functions.feed2vec(bookList[i], tokenize = True)\n",
    "        sep_token_list_of_lists.append(sep_book_list[0])\n",
    "        print(\"token list of book \", i+1, \"is \", sep_book_list[0])\n",
    "        print(\"tokens in book \", i+1, \"is\", len(sep_book_list[0]))\n",
    "        print()\n",
    "        for word in sep_book_list[0]:\n",
    "            token_list.append(word)  #use true here\n",
    "        print(\"data type of token_list[\", i+1, \"]\", type(token_list[i]))\n",
    "\n",
    "    print()\n",
    "    print(\"length of vocab \", len(token_list))\n",
    "    \n",
    "    if separate == False:\n",
    "        print(\"final list length/token count \", len(token_list))\n",
    "        print (\"all tokens \", token_list)\n",
    "        return token_list\n",
    "    else:\n",
    "        print(\"list of all tokens (list of list form) \", sep_token_list_of_lists)\n",
    "        return sep_token_list_of_lists\n",
    "    print()\n",
    "\n",
    "\n",
    "#local book path = C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files   this format is what works, then use .json as type  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a file location C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\n",
      "Enter a data type(.json please) .json\n",
      "Is there an additional file type? (y or n)  y\n",
      "Enter a second data type(.txt)  .txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookCount is  7\n",
      "book list is  ['C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Polymer_Chemistry_by_Koltzsenburg_Maskos_and_Nuyken.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Polymer_Synthesis_Theory_and_Practice_by_Braun_Cherdron_Rehahn_Ritter_and_Voit.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Book Basic Principles of Organic Chemistry (Roberts and Caserio).txt', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Book Logic of Organic Synthesis (Rao).txt', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Book Organic Chemistry - A Carbonyl Early Approach (McMichael).txt', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Soderberg_bio_o_chem.txt']\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json\n",
      "                                                   0\n",
      "0  michael mosher · kenneth trantham  brewing  sc...\n",
      "1  all rights are reserved by the publisher, whet...\n",
      "2  the use of general descriptive names, register...\n",
      "3  the publisher, the authors and the editors are...\n",
      "4  neither the publisher nor the authors or the e...\n",
      "5                     trademarks, service marks, etc\n",
      "6   printed on acid-free paper  this springer imp...\n",
      "7  thus began the discussion for the beginnings o...\n",
      "8  “wouldn’t it be awesome,” we thought, “if we c...\n",
      "9              the result is what you will find here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:35:49: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 14:35:49: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  1 is  ['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '4', 'neither', 'publisher', 'authors', 'editors', 'give', 'warrant', '.', '.', '.', '.', '.', '.', '131', '##7', '##6', '131', '##7', '##7', 'k', 'g', 'k', 'j', 'k', 'p', 'r', 'n', 'e', 'g', 'k', 'j', 'k', 'p', 'l', 'h', 'n', 'e', 'g', 'k', 'j', 'k', 'g', '.', '.', '.', '131', '##7', '##8', 'b', 'e', 'l', 'b', 'appendix', 'b', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '.', '.', '.', '131', '##7', '##9', 'moshe', '##r', 'k', '131', '##80', 'tran', '##tham', 'brewing', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '81', '##7', '##9', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  1 is 146\n",
      "\n",
      "data type of token_list[ 1 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Polymer_Chemistry_by_Koltzsenburg_Maskos_and_Nuyken.json\n",
      "                                                   0\n",
      "0  sebastian koltzenburg michael maskos oskar nuy...\n",
      "1  all rights are reserved by the publisher, whet...\n",
      "2  the  use  of  general  descriptive  names,  re...\n",
      "3   in  this  publication does not imply, even in...\n",
      "4  the publisher, the authors and the editors are...\n",
      "5   neither  the  publisher  nor  the  authors or...\n",
      "6   cover illustration: with kind permission by g...\n",
      "7   all computer chips used in our desktops, lapt...\n",
      "8   cutting-edge  biomedical  applications  requi...\n",
      "9  the interior of every automobile is almost ent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:35:49: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 14:35:49: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  2 is  ['0', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'michael', 'mask', '##os', 'oskar', 'nu', '##y', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '.', '.', '.', '4', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '.', '.', '.', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '.', '.', '.', '950', '##5', 'gut', '##ta', 'perch', '##a', 'h', 'ha', '##f', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'h', '.', '.', '.', '950', '##6', 'see', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', 'pe', '##llet', '##izing', 'cold', 'pe', '##ll', '.', '.', '.', '950', '##7', 'see', 'forming', 'processes', 'resin', 'transfer', 'mold', '##ing', 'r', '.', '.', '.', '950', '##8', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', '##er', 'transit', '##io', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '86', '##8', '##2', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  2 is 162\n",
      "\n",
      "data type of token_list[ 2 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Polymer_Synthesis_Theory_and_Practice_by_Braun_Cherdron_Rehahn_Ritter_and_Voit.json\n",
      "                                                   0\n",
      "0          polymer synthesis: theory and practice   \n",
      "1    dietrich braun (29) harald cherdron matthias...\n",
      "2  dresden germany  isbn 978-3-642-28979-8 doi 10...\n",
      "3  all rights are reserved by the publisher, whet...\n",
      "4  exempted from this legal reservation are brief...\n",
      "5  duplication of this publication or parts there...\n",
      "6  permissions for use may be obtained through ri...\n",
      "7  violations are liable to prosecution under the...\n",
      "8  the use of general descriptive names, register...\n",
      "9  in this publication does not imply, even in th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:35:50: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 14:35:50: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  3 is  ['0', 'polymer', 'synthesis', 'theory', 'practice', '1', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', 're', '##ha', '##hn', '.', '.', '.', '2', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'n', '.', '.', '.', '3', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '4', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '.', '.', '.', '.', '.', '.', '122', '##60', 'braun', 'et', 'al', 'polymer', 'synthesis', 'theory', 'practice', '.', '.', '.', '122', '##6', '##1', 'see', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', '##r', 'of', '##et', 'ole', '##d', '122', '##6', '##2', 'see', 'organic', 'light', 'emi', '##tting', 'di', '##ode', 'ole', '##d', 'ol', '##igo', '##mer', '.', '.', '.', '122', '##6', '##3', 'see', 'organic', 'photo', 'voltage', 'op', '##v', 'organic', 'field', '##ef', '##f', '.', '.', '.', '122', '##64', 'see', 'polymer', 'based', 'organic', 'light', 'emi', '##tting', 'di', '##ode', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '70', '##57', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  3 is 143\n",
      "\n",
      "data type of token_list[ 3 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Book Basic Principles of Organic Chemistry (Roberts and Caserio).txt\n",
      "                                                   0\n",
      "0  1: Introduction to Organic Chemistry: You now ...\n",
      "1  2: Structural Organic Chemistry: This chapter ...\n",
      "2  3: Organic Nomenclature: A chemical nomenclatu...\n",
      "3  4: Alkanes: Alkanes are the simplest organic m...\n",
      "4  5: Stereoisomerism of Organic Molecules: Posit...\n",
      "5  6: Bonding in Organic Molecules: Remembering t...\n",
      "6  7: Other Compounds than Hydrocarbons: We begin...\n",
      "7  8: Nucleophilic Substitution and Elimination R...\n",
      "8  9: Separation, Purification, & Identification ...\n",
      "9  10: Alkenes and Alkynes I - Ionic and Radical ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:35:51: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 14:35:51: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  4 is  ['0', 'introduction', 'organic', 'chemistry', 'starting', 'study', '.', '.', '.', '1', 'structural', 'organic', 'chemistry', 'chapter', 'briefly', 'r', '.', '.', '.', '2', 'organic', 'nomenclature', 'chemical', 'nomenclature', 'set', '.', '.', '.', '3', 'al', '##kan', '##es', 'al', '##kan', '##es', 'simplest', 'organic', 'molecules', 'con', '.', '.', '.', '4', 'stereo', '##iso', '##mer', '##ism', 'organic', 'molecules', 'position', 'iso', '.', '.', '.', '.', '.', '.', '71', '##50', 'show', 'mechanisms', 'combination', 'others', 'described', 'c', '.', '.', '.', '71', '##51', 'ch', 'ch', 'pd', '##cl', 'h', 'ch', '71', '##52', 'reaction', 'used', 'large', '##sca', '##le', 'production', 'ox', '##idi', '##zing', '.', '.', '.', '71', '##53', 'b', 'balance', 'competitive', 'nu', '##cle', '##op', '##hil', '##ic', 'reactions', 'd', '.', '.', '.', '71', '##54', 'write', 'me', '##chan', '##istic', 'steps', 'account', 'difference', 'ste', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '67', '##42', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  4 is 131\n",
      "\n",
      "data type of token_list[ 4 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Book Logic of Organic Synthesis (Rao).txt\n",
      "                                                   0\n",
      "0                  1: Synthesis of Organic Molecules\n",
      "1  2: Rules and Guidelines Governing Organic Synt...\n",
      "2          Baldwin’s Rule for Ring Closure Reactions\n",
      "3                                       Bredt's Rule\n",
      "4                      Cram's Rule and Prelog's Rule\n",
      "5                  Hofmann’s Rule and Zaitsev’s Rule\n",
      "6                                   Markovnikov Rule\n",
      "7   3: Criteria for Selection of the Synthetic Route\n",
      "8                          4: The Logic of Synthesis\n",
      "9              5: Strategies in Disparlure Synthesis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:35:52: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 14:35:52: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  5 is  ['0', 'synthesis', 'organic', 'molecules', '1', 'rules', 'guidelines', 'governing', 'organic', 'synthesis', '2', 'baldwin', '’', 's', 'rule', 'ring', 'closure', 'reactions', '3', 'bred', '##ts', 'rule', '4', 'cr', '##ams', 'rule', 'pre', '##log', '##s', 'rule', '.', '.', '.', '580', 'fig', '58', '##1', 'use', 'transition', 'metal', 're', '##age', '##nts', 'could', 'aid', 'con', '##st', '##r', '.', '.', '.', '58', '##2', 'fig', '58', '##3', 'rare', 'molecular', 'engineering', 'marvel', '##s', 'like', 'rb', 'woo', '.', '.', '.', '58', '##4', 'doubt', 'feasibility', 'protection', 'free', 'synthesis', 'co', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '49', '##9', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  5 is 91\n",
      "\n",
      "data type of token_list[ 5 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Book Organic Chemistry - A Carbonyl Early Approach (McMichael).txt\n",
      "                                                   0\n",
      "0    1: Carbonyl Group: Notation, Structure, Bonding\n",
      "1        2: Functional Groups, Hybridization, Naming\n",
      "2       3: Additions: Electrophilic and Nucleophilic\n",
      "3          4: Acetal Formation, Mechanism, Resonance\n",
      "4         5: Nitrogen Nucleophiles - Imine Formation\n",
      "5          6: Addition of Organometallics - Grignard\n",
      "6        7: Oxidation & Reduction, alpha-C-H acidity\n",
      "7         8: Enolates, Aldol Condensation, Synthesis\n",
      "8    9: Carboxylic Acid Derivatives: Interconversion\n",
      "9  10: Carboxylic Acid Derivatives - Alpha Carbon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:35:52: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 14:35:52: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  6 is  ['0', 'carbon', '##yl', 'group', 'notation', 'structure', 'bonding', '1', 'functional', 'groups', 'hybrid', '##ization', 'naming', '2', 'additions', 'electro', '##phi', '##lic', 'nu', '##cle', '##op', '##hil', '##ic', '3', 'ace', '##tal', 'formation', 'mechanism', 'resonance', '4', 'nitrogen', 'nu', '##cle', '##op', '##hil', '##es', 'im', '##ine', 'formation', '.', '.', '.', '101', '##8', 'finally', 'lets', 'return', 'another', 'application', 'free', 'r', '.', '.', '.', '102', '##0', 'new', 'molecule', 'would', 'also', 'free', 'radical', 'could', 'add', '.', '.', '.', '102', '##1', 'polymer', '##ization', 'reactions', 'also', 'important', 'table', '.', '.', '.', '102', '##3', 'since', 'repeating', 'unit', 'formed', 'adding', 'end', 'double', '.', '.', '.', '102', '##4', 'kirk', 'mc', '##mic', '##hae', '##l', 'washington', 'state', 'university', 'name', ':', 'text', ',', 'length', ':', '84', '##8', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  6 is 113\n",
      "\n",
      "data type of token_list[ 6 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Soderberg_bio_o_chem.txt\n",
      "                                                   0\n",
      "0  1: Introduction to Organic Structure and Bondi...\n",
      "1  2: Introduction to Organic Structure and Bondi...\n",
      "2               3: Conformations and Stereochemistry\n",
      "3  4: Structure Determination I- UV-Vis and Infra...\n",
      "4  5: Structure Determination Part II - Nuclear M...\n",
      "5                  6: Overview of Organic Reactivity\n",
      "6                             7: Acid-base Reactions\n",
      "7             8: Nucleophilic Substitution Reactions\n",
      "8  9: Phosphate Transfer Reactions: This chapter ...\n",
      "9       10: Nucleophilic Carbonyl Addition Reactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:35:53: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 14:35:53: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  7 is  ['0', 'introduction', 'organic', 'structure', 'bonding', 'chapter', '.', '.', '.', '1', 'introduction', 'organic', 'structure', 'bonding', 'ii', '2', 'conform', '##ations', 'stereo', '##chemist', '##ry', '3', 'structure', 'determination', 'uv', '##vis', 'infrared', 'spec', '##tro', '.', '.', '.', '4', 'structure', 'determination', 'part', 'ii', 'nuclear', 'magnet', '.', '.', '.', '.', '.', '.', '425', '##6', 'phase', '425', '##7', 'phase', '425', '##8', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', '425', '##9', 'completing', 'chapter', 'able', '42', '##60', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '40', '##8', '##1', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  7 is 94\n",
      "\n",
      "data type of token_list[ 7 ] <class 'str'>\n",
      "\n",
      "length of vocab  880\n",
      "list of token lists  [['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '4', 'neither', 'publisher', 'authors', 'editors', 'give', 'warrant', '.', '.', '.', '.', '.', '.', '131', '##7', '##6', '131', '##7', '##7', 'k', 'g', 'k', 'j', 'k', 'p', 'r', 'n', 'e', 'g', 'k', 'j', 'k', 'p', 'l', 'h', 'n', 'e', 'g', 'k', 'j', 'k', 'g', '.', '.', '.', '131', '##7', '##8', 'b', 'e', 'l', 'b', 'appendix', 'b', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '.', '.', '.', '131', '##7', '##9', 'moshe', '##r', 'k', '131', '##80', 'tran', '##tham', 'brewing', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '81', '##7', '##9', ',', 'dt', '##ype', ':', 'object'], ['0', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'michael', 'mask', '##os', 'oskar', 'nu', '##y', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '.', '.', '.', '4', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '.', '.', '.', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '.', '.', '.', '950', '##5', 'gut', '##ta', 'perch', '##a', 'h', 'ha', '##f', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'h', '.', '.', '.', '950', '##6', 'see', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', 'pe', '##llet', '##izing', 'cold', 'pe', '##ll', '.', '.', '.', '950', '##7', 'see', 'forming', 'processes', 'resin', 'transfer', 'mold', '##ing', 'r', '.', '.', '.', '950', '##8', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', '##er', 'transit', '##io', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '86', '##8', '##2', ',', 'dt', '##ype', ':', 'object'], ['0', 'polymer', 'synthesis', 'theory', 'practice', '1', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', 're', '##ha', '##hn', '.', '.', '.', '2', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'n', '.', '.', '.', '3', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '4', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '.', '.', '.', '.', '.', '.', '122', '##60', 'braun', 'et', 'al', 'polymer', 'synthesis', 'theory', 'practice', '.', '.', '.', '122', '##6', '##1', 'see', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', '##r', 'of', '##et', 'ole', '##d', '122', '##6', '##2', 'see', 'organic', 'light', 'emi', '##tting', 'di', '##ode', 'ole', '##d', 'ol', '##igo', '##mer', '.', '.', '.', '122', '##6', '##3', 'see', 'organic', 'photo', 'voltage', 'op', '##v', 'organic', 'field', '##ef', '##f', '.', '.', '.', '122', '##64', 'see', 'polymer', 'based', 'organic', 'light', 'emi', '##tting', 'di', '##ode', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '70', '##57', ',', 'dt', '##ype', ':', 'object'], ['0', 'introduction', 'organic', 'chemistry', 'starting', 'study', '.', '.', '.', '1', 'structural', 'organic', 'chemistry', 'chapter', 'briefly', 'r', '.', '.', '.', '2', 'organic', 'nomenclature', 'chemical', 'nomenclature', 'set', '.', '.', '.', '3', 'al', '##kan', '##es', 'al', '##kan', '##es', 'simplest', 'organic', 'molecules', 'con', '.', '.', '.', '4', 'stereo', '##iso', '##mer', '##ism', 'organic', 'molecules', 'position', 'iso', '.', '.', '.', '.', '.', '.', '71', '##50', 'show', 'mechanisms', 'combination', 'others', 'described', 'c', '.', '.', '.', '71', '##51', 'ch', 'ch', 'pd', '##cl', 'h', 'ch', '71', '##52', 'reaction', 'used', 'large', '##sca', '##le', 'production', 'ox', '##idi', '##zing', '.', '.', '.', '71', '##53', 'b', 'balance', 'competitive', 'nu', '##cle', '##op', '##hil', '##ic', 'reactions', 'd', '.', '.', '.', '71', '##54', 'write', 'me', '##chan', '##istic', 'steps', 'account', 'difference', 'ste', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '67', '##42', ',', 'dt', '##ype', ':', 'object'], ['0', 'synthesis', 'organic', 'molecules', '1', 'rules', 'guidelines', 'governing', 'organic', 'synthesis', '2', 'baldwin', '’', 's', 'rule', 'ring', 'closure', 'reactions', '3', 'bred', '##ts', 'rule', '4', 'cr', '##ams', 'rule', 'pre', '##log', '##s', 'rule', '.', '.', '.', '580', 'fig', '58', '##1', 'use', 'transition', 'metal', 're', '##age', '##nts', 'could', 'aid', 'con', '##st', '##r', '.', '.', '.', '58', '##2', 'fig', '58', '##3', 'rare', 'molecular', 'engineering', 'marvel', '##s', 'like', 'rb', 'woo', '.', '.', '.', '58', '##4', 'doubt', 'feasibility', 'protection', 'free', 'synthesis', 'co', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '49', '##9', ',', 'dt', '##ype', ':', 'object'], ['0', 'carbon', '##yl', 'group', 'notation', 'structure', 'bonding', '1', 'functional', 'groups', 'hybrid', '##ization', 'naming', '2', 'additions', 'electro', '##phi', '##lic', 'nu', '##cle', '##op', '##hil', '##ic', '3', 'ace', '##tal', 'formation', 'mechanism', 'resonance', '4', 'nitrogen', 'nu', '##cle', '##op', '##hil', '##es', 'im', '##ine', 'formation', '.', '.', '.', '101', '##8', 'finally', 'lets', 'return', 'another', 'application', 'free', 'r', '.', '.', '.', '102', '##0', 'new', 'molecule', 'would', 'also', 'free', 'radical', 'could', 'add', '.', '.', '.', '102', '##1', 'polymer', '##ization', 'reactions', 'also', 'important', 'table', '.', '.', '.', '102', '##3', 'since', 'repeating', 'unit', 'formed', 'adding', 'end', 'double', '.', '.', '.', '102', '##4', 'kirk', 'mc', '##mic', '##hae', '##l', 'washington', 'state', 'university', 'name', ':', 'text', ',', 'length', ':', '84', '##8', ',', 'dt', '##ype', ':', 'object'], ['0', 'introduction', 'organic', 'structure', 'bonding', 'chapter', '.', '.', '.', '1', 'introduction', 'organic', 'structure', 'bonding', 'ii', '2', 'conform', '##ations', 'stereo', '##chemist', '##ry', '3', 'structure', 'determination', 'uv', '##vis', 'infrared', 'spec', '##tro', '.', '.', '.', '4', 'structure', 'determination', 'part', 'ii', 'nuclear', 'magnet', '.', '.', '.', '.', '.', '.', '425', '##6', 'phase', '425', '##7', 'phase', '425', '##8', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', '425', '##9', 'completing', 'chapter', 'able', '42', '##60', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '40', '##8', '##1', ',', 'dt', '##ype', ':', 'object']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:35:53: collecting all words and their counts\n",
      "INFO - 14:35:53: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 14:35:53: collected 382 word types from a corpus of 880 raw words and 7 sentences\n",
      "INFO - 14:35:53: Loading a fresh vocabulary\n",
      "INFO - 14:35:53: effective_min_count=1 retains 382 unique words (100% of original 382, drops 0)\n",
      "INFO - 14:35:53: effective_min_count=1 leaves 880 word corpus (100% of original 880, drops 0)\n",
      "INFO - 14:35:53: deleting the raw counts dictionary of 382 items\n",
      "INFO - 14:35:53: sample=0.001 downsamples 74 most-common words\n",
      "INFO - 14:35:53: downsampling leaves estimated 584 word corpus (66.5% of prior 880)\n",
      "INFO - 14:35:53: estimated required memory for 382 words and 500 dimensions: 1719000 bytes\n",
      "INFO - 14:35:53: resetting layer weights\n",
      "INFO - 14:35:53: training model with 3 workers on 382 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 1 : training on 880 raw words (580 effective words) took 0.0s, 103686 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 2 : training on 880 raw words (580 effective words) took 0.0s, 136109 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 3 : training on 880 raw words (587 effective words) took 0.0s, 168712 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 4 : training on 880 raw words (576 effective words) took 0.0s, 234346 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 5 : training on 880 raw words (598 effective words) took 0.0s, 149399 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 6 : training on 880 raw words (594 effective words) took 0.0s, 243612 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 7 : training on 880 raw words (592 effective words) took 0.0s, 170605 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 8 : training on 880 raw words (584 effective words) took 0.0s, 146252 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 9 : training on 880 raw words (586 effective words) took 0.0s, 166933 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 10 : training on 880 raw words (597 effective words) took 0.0s, 229943 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 11 : training on 880 raw words (574 effective words) took 0.0s, 146807 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 12 : training on 880 raw words (575 effective words) took 0.0s, 178151 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 13 : training on 880 raw words (583 effective words) took 0.0s, 199952 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 14 : training on 880 raw words (587 effective words) took 0.0s, 154230 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 15 : training on 880 raw words (590 effective words) took 0.0s, 181874 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 16 : training on 880 raw words (593 effective words) took 0.0s, 185214 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 17 : training on 880 raw words (590 effective words) took 0.0s, 148630 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 18 : training on 880 raw words (577 effective words) took 0.0s, 178837 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 19 : training on 880 raw words (585 effective words) took 0.0s, 191583 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 20 : training on 880 raw words (588 effective words) took 0.0s, 154367 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 21 : training on 880 raw words (574 effective words) took 0.0s, 198129 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 22 : training on 880 raw words (577 effective words) took 0.0s, 169816 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 23 : training on 880 raw words (585 effective words) took 0.0s, 168331 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 24 : training on 880 raw words (578 effective words) took 0.0s, 189347 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 25 : training on 880 raw words (590 effective words) took 0.0s, 203983 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 26 : training on 880 raw words (587 effective words) took 0.0s, 151285 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 27 : training on 880 raw words (583 effective words) took 0.0s, 199439 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 28 : training on 880 raw words (598 effective words) took 0.0s, 185553 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 29 : training on 880 raw words (600 effective words) took 0.0s, 167136 effective words/s\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:35:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:35:53: EPOCH - 30 : training on 880 raw words (592 effective words) took 0.0s, 209372 effective words/s\n",
      "INFO - 14:35:53: training on a 26400 raw words (17580 effective words) took 0.2s, 104918 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made it here!\n",
      "Time to train the model: 0.0 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:35:53: saving Word2VecKeyedVectors object under C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv, separately None\n",
      "INFO - 14:35:53: not storing attribute vectors_norm\n",
      "INFO - 14:35:53: saved C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv\n",
      "INFO - 14:35:53: loading Word2VecKeyedVectors object from C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv\n",
      "INFO - 14:35:53: setting ignored attribute vectors_norm to None\n",
      "INFO - 14:35:53: loaded C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus count is  7\n",
      "epochs is  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:35:53: saving Word2Vec object under fourBook, separately None\n",
      "INFO - 14:35:53: not storing attribute vectors_norm\n",
      "INFO - 14:35:53: not storing attribute cum_table\n",
      "INFO - 14:35:53: saved fourBook\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyed vectors  <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x000001C2755A3508>\n",
      "all_vectors(done via vocab)  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_vectors type is  <class 'list'>\n",
      "all words in wv.vocab are  ['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '2', 'use', 'general', 'descriptive', 'names', 'registered', '3', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '4', 'neither', 'give', 'warrant', '131', '##7', '##6', 'k', 'g', 'j', 'p', 'r', 'n', 'e', 'l', 'h', '##8', 'b', 'appendix', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '##9', '##80', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', 'name', ':', 'text', ',', 'length', '81', 'dt', '##ype', 'object', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'mask', '##os', 'oskar', 'nu', '##y', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '##5', 'gut', '##ta', 'perch', '##a', 'ha', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', '##llet', '##izing', 'cold', '##ll', 'forming', 'processes', 'resin', 'transfer', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', 'transit', '##io', '86', '##2', 'polymer', 'synthesis', 'theory', 'practice', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', '##ha', '##hn', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '122', '##60', 'et', 'al', '##1', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', 'of', '##et', 'ole', 'light', 'emi', '##tting', 'di', '##ode', 'ol', '##igo', '##mer', '##3', 'photo', 'voltage', 'op', '##v', '##64', 'based', '70', '##57', 'introduction', 'chemistry', 'starting', 'study', 'structural', 'chapter', 'briefly', 'nomenclature', 'chemical', 'set', '##kan', '##es', 'simplest', 'molecules', 'stereo', '##iso', '##ism', 'position', 'iso', '71', '##50', 'show', 'mechanisms', 'combination', 'others', 'described', 'c', '##51', 'ch', 'pd', '##cl', '##52', 'reaction', 'used', 'large', '##sca', '##le', 'production', 'ox', '##idi', '##zing', '##53', 'balance', 'competitive', '##cle', '##op', '##hil', '##ic', 'reactions', 'd', '##54', 'write', 'me', '##chan', '##istic', 'steps', 'account', 'difference', 'ste', '67', '##42', 'rules', 'guidelines', 'governing', 'baldwin', 'rule', 'ring', 'closure', 'bred', '##ts', 'cr', '##ams', 'pre', '##log', '##s', '580', 'fig', '58', 'transition', 'metal', '##nts', 'could', 'aid', '##st', 'rare', 'molecular', 'engineering', 'marvel', 'like', 'rb', 'woo', 'doubt', 'feasibility', 'protection', 'free', 'co', '49', 'carbon', '##yl', 'group', 'notation', 'structure', 'bonding', 'functional', 'groups', 'hybrid', '##ization', 'naming', 'additions', 'electro', '##phi', '##lic', 'ace', '##tal', 'formation', 'mechanism', 'resonance', 'nitrogen', 'im', '##ine', '101', 'finally', 'lets', 'return', 'another', 'application', '102', '##0', 'new', 'molecule', 'would', 'also', 'radical', 'add', 'important', 'since', 'repeating', 'unit', 'formed', 'adding', 'end', 'double', 'kirk', 'mc', '##mic', '##hae', '##l', 'washington', 'university', '84', 'ii', 'conform', '##ations', '##chemist', '##ry', 'determination', 'uv', '##vis', 'infrared', 'spec', '##tro', 'nuclear', 'magnet', '425', 'phase', 'biological', 'emphasis', 'tim', 'so', '##de', 'completing', 'able', '42', '40']\n",
      "number of words in wv.vocab is  382\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This function needs to take in a list of tokens, set them as the global tokens value for Louisa's functions then train the model with them and return\n",
    "the word embeddings\n",
    "\"\"\"\n",
    "\n",
    "#take in a vocab list\n",
    "\n",
    "list_of_tokens = super_list_maker()\n",
    "#saveName = input(\"Enter a name for the model saved: \")\n",
    "w2v_functions.tokens = list_of_tokens\n",
    "all_vecs = w2v_functions.w2v_train(\"fourBook\", min_count = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type of all_vecs is  <class 'list'>\n",
      "382\n",
      "[-1.71951996e-03 -2.19952804e-03  2.83069513e-03 -2.13265214e-02\n",
      "  9.65994317e-03  2.80566560e-03  3.26305162e-03  6.57500140e-03\n",
      " -3.85645707e-03  5.34121791e-05 -8.63810856e-05  5.24376566e-03\n",
      "  1.21376468e-02 -3.98881547e-03  6.67578657e-04  1.36011802e-02\n",
      " -3.28594795e-03  3.70483194e-03 -1.37689188e-02 -4.65481123e-03\n",
      " -1.31360572e-02 -8.54541548e-03 -8.33907537e-03  8.45926814e-03\n",
      " -5.79611864e-04 -3.71198053e-03 -9.51646362e-04  2.39330227e-03\n",
      "  2.01878860e-03 -1.35969243e-03 -1.26926927e-02 -8.05184804e-03\n",
      "  7.24444585e-03  4.60135890e-03  6.75529568e-03  9.17113945e-03\n",
      "  2.46609095e-04 -4.45088744e-03  5.79753565e-03 -1.03463242e-02\n",
      "  8.01709946e-03  8.50031385e-04 -1.37296189e-02  1.86680490e-03\n",
      " -9.97264124e-03  3.48948000e-04 -1.50616933e-02 -4.92462190e-03\n",
      "  1.47618921e-02  5.83594153e-03 -7.82195292e-03  5.92956552e-03\n",
      " -1.71549851e-03  8.62253923e-03 -7.12301722e-03 -7.27313245e-03\n",
      "  1.16259214e-02  4.45323205e-03 -1.14657935e-02 -1.09305428e-02\n",
      " -2.60979729e-03  9.69728921e-03  6.45490736e-03 -1.82016997e-03\n",
      " -1.03123123e-02  2.47136806e-03 -3.25840106e-03 -1.47593450e-02\n",
      "  1.27696730e-02 -1.09104235e-02 -7.85105117e-03  9.89269838e-03\n",
      "  1.19776954e-03 -2.00610105e-02  2.72759353e-03  1.07960422e-02\n",
      " -1.29494164e-02 -8.57900176e-03 -1.19468328e-02 -7.14177731e-04\n",
      " -3.59006273e-03 -1.11942319e-03 -2.20999913e-03 -6.21484080e-03\n",
      " -1.05236832e-03  4.80388245e-03  9.86102782e-03 -7.39733595e-03\n",
      " -3.60759697e-03 -1.65067259e-02  1.35432696e-02 -1.63576682e-04\n",
      " -1.04657346e-02 -1.00644759e-03 -9.94892512e-03 -7.64171639e-03\n",
      " -5.86008234e-03 -5.39287983e-04 -8.22315272e-03 -2.20301356e-02\n",
      " -5.88748185e-03  8.65309499e-03 -1.33586330e-02 -1.81707076e-03\n",
      "  6.30629621e-03 -6.51543215e-03  5.90125192e-03 -3.71188833e-03\n",
      "  5.76663297e-03  2.31510261e-03  3.24651343e-03  9.75712016e-03\n",
      " -4.32108995e-03 -1.47629681e-03 -1.72748696e-04  6.62037777e-03\n",
      " -9.72602703e-03  6.35490194e-03 -1.51911541e-03  3.50832520e-03\n",
      "  6.31555542e-03  3.28835729e-03 -6.42204005e-03  7.68820476e-03\n",
      "  1.42979715e-03  4.37604124e-03 -3.52163496e-03 -3.18604824e-03\n",
      " -1.38942180e-02  6.77178334e-03 -4.73458553e-03 -1.38065582e-02\n",
      " -1.62032375e-03  7.92627747e-04  2.52504647e-03  8.95929523e-03\n",
      " -7.84360524e-03  5.44690574e-03 -3.68787791e-04 -1.09919589e-02\n",
      " -3.66426772e-03 -6.40510675e-03 -9.09208495e-04  5.01694065e-03\n",
      "  1.25094911e-03  1.93519390e-03 -6.85886480e-03 -8.06122180e-03\n",
      " -7.78795034e-03  7.97052309e-03  8.31808429e-03  1.14957488e-03\n",
      "  4.26728977e-03 -2.39196257e-03  2.41664285e-03  9.40216519e-03\n",
      " -1.32537605e-02  2.02883361e-03 -6.86352421e-03 -1.15773629e-03\n",
      "  2.20276583e-02 -4.57257265e-03 -4.11886908e-03  1.99880041e-02\n",
      " -1.27167506e-02  1.96130350e-02  8.20828788e-03 -6.58837287e-03\n",
      "  7.05301994e-03 -1.87342241e-03  3.49552534e-03 -1.68515611e-02\n",
      " -2.92294480e-05 -1.28111998e-02  2.97268759e-03  1.47602344e-02\n",
      " -7.37417862e-03 -1.67963468e-02 -5.40126069e-03  1.35015007e-02\n",
      " -7.47756846e-03  1.56251471e-02 -6.72261324e-03 -6.68417010e-03\n",
      "  7.48403894e-04  1.31843230e-02 -1.66277997e-02 -9.39744059e-03\n",
      "  6.18566712e-03 -1.43568974e-03 -3.74600175e-03 -5.52227208e-03\n",
      " -1.14329848e-02 -1.78868428e-03  2.54623522e-03 -1.80212203e-02\n",
      "  5.30276587e-03 -8.70792894e-04 -1.63546670e-03  8.83623550e-04\n",
      "  9.64146480e-03 -1.83781283e-03 -4.22382727e-03  5.74121426e-04\n",
      " -1.85931548e-02  6.05691131e-03  1.60792079e-02 -5.92176849e-03\n",
      "  7.94765912e-03  5.60165942e-03 -1.14223491e-02  3.64633487e-03\n",
      "  1.87086857e-06  4.71565640e-03  2.14841007e-03  1.08840792e-02\n",
      "  5.51364850e-04 -4.95876279e-03 -1.39073906e-02  7.18212966e-03\n",
      "  1.36317797e-02 -1.34664141e-02  4.12826193e-03  2.98295729e-03\n",
      " -2.70782784e-02  1.60406362e-02  2.86817737e-03 -2.26020682e-04\n",
      "  1.03379572e-02  4.36839275e-03 -1.20023382e-03  2.09072419e-02\n",
      "  9.89574939e-04  3.71459918e-03  1.09411646e-02 -7.84414168e-03\n",
      "  9.85717680e-03  9.19228885e-03  6.42598141e-03  5.00186207e-03\n",
      " -1.49510978e-02  3.30300769e-03  9.63595416e-03 -4.18770965e-03\n",
      " -6.74797548e-03  1.03010396e-02  4.07384895e-03 -1.17358507e-03\n",
      "  1.58385118e-03  2.99439579e-03 -1.57524012e-02 -2.77362019e-03\n",
      " -1.51082529e-02 -1.16948038e-02  3.74346203e-03  1.88244209e-02\n",
      "  5.40892710e-04 -2.10058456e-03  9.92408302e-03  3.63644212e-03\n",
      " -2.46839435e-03 -8.76286998e-03  8.51623714e-03  1.13860723e-02\n",
      "  8.18549749e-03  3.03194905e-03 -7.49418279e-03  7.49438128e-04\n",
      "  5.25474316e-03  9.39138606e-03 -1.50474897e-02 -6.70871790e-03\n",
      "  1.08188705e-03  6.65268628e-03 -5.31960139e-03 -9.87324584e-03\n",
      " -5.36088739e-03 -5.93998423e-03  8.84635665e-04 -1.65730454e-02\n",
      "  2.17692787e-03 -8.68537463e-03 -6.72232779e-03 -8.94665904e-03\n",
      " -9.89831705e-03  1.22834099e-02  1.20110554e-03  5.31261927e-03\n",
      "  7.43674347e-03 -2.09577139e-02  1.16755790e-03 -5.75133832e-03\n",
      " -2.02861130e-02  1.36592390e-03 -2.18264689e-03 -3.54678254e-03\n",
      " -1.43256858e-02 -1.99622917e-03  9.20128077e-03 -1.08940685e-02\n",
      "  8.31272732e-03  3.20376246e-04  6.38656830e-03 -2.22593173e-03\n",
      " -6.21937029e-03  1.32898279e-02  7.37020920e-04  3.61443707e-03\n",
      " -3.34498170e-03  3.40987486e-03  8.08798894e-03 -1.83786009e-03\n",
      " -1.07031288e-02 -1.20097408e-02 -1.04802884e-02 -1.86413771e-03\n",
      " -2.30962830e-03 -1.38187157e-02  1.59851220e-02  1.15321539e-02\n",
      " -1.12515129e-02 -3.30264959e-03  1.56256687e-02  6.92177983e-03\n",
      "  2.33427412e-03  9.52636357e-03 -6.35132100e-03 -4.39571403e-03\n",
      " -1.30649016e-03 -1.27104372e-02  6.07082387e-03  6.65804977e-03\n",
      " -5.35515894e-04 -9.14943404e-03 -1.41355535e-02 -1.96612836e-03\n",
      "  9.05069429e-03 -6.82725804e-03 -3.71892191e-03 -6.95518451e-03\n",
      "  3.20227677e-03  5.87083912e-03  3.92036792e-03 -5.80435712e-03\n",
      "  6.19159546e-03  4.27976670e-03 -5.79937175e-03 -3.40536004e-03\n",
      " -3.60620674e-04 -1.55042773e-02 -1.38444165e-02 -1.10295229e-02\n",
      " -3.18901660e-03 -1.01637663e-02 -9.47114720e-04  9.07399692e-03\n",
      " -9.70257819e-03  4.24682116e-03 -9.58355237e-03  2.54066288e-03\n",
      "  9.37334262e-03  6.04184531e-03 -3.17768333e-03  1.79177709e-02\n",
      "  1.05530685e-02 -5.74654958e-04  1.03751263e-04  2.14071162e-02\n",
      "  1.37138111e-03 -6.11036224e-03  3.73386219e-03 -2.46053049e-03\n",
      "  5.06213354e-03 -1.94238557e-03  1.70017555e-02 -4.24129795e-03\n",
      " -2.55870307e-03 -9.58939269e-03  1.88157894e-02 -3.91587475e-03\n",
      "  1.26786828e-02  1.06060859e-02 -1.94039140e-02  6.77333819e-03\n",
      " -1.71678106e-03 -6.05545519e-03 -8.28482618e-04 -5.56908082e-03\n",
      "  4.91068466e-03 -8.34409380e-04 -4.26590117e-03  8.80214793e-04\n",
      " -5.66770788e-03 -5.19801397e-03 -5.86357852e-03 -9.60785022e-04\n",
      " -8.92073382e-03 -6.09186292e-03 -7.33341975e-03 -5.95296314e-03\n",
      "  8.22463445e-03  3.31389741e-03  2.80725979e-03 -6.95725263e-04\n",
      "  3.81637044e-04 -5.26646897e-03  4.32063080e-03 -3.08005931e-03\n",
      " -8.61588574e-04 -1.14870945e-03  1.34587521e-02  2.56413524e-03\n",
      "  4.94671380e-03  4.67598625e-03  7.57635979e-04  3.16072052e-04\n",
      "  1.16708381e-02  8.09181191e-04 -3.71390115e-03 -1.85288806e-02\n",
      "  4.20802040e-03  1.57558022e-03  1.63879357e-02  1.67253744e-02\n",
      "  5.03092422e-04  6.88402157e-04 -1.22716092e-02 -2.30829479e-04\n",
      " -1.42257847e-02 -6.68379851e-03 -8.65271874e-03 -7.42490310e-03\n",
      " -3.58328782e-03 -7.03980541e-03 -1.63614918e-02 -3.07201152e-03\n",
      "  1.55436894e-04  2.93709547e-03  1.02383653e-02  3.59761715e-03\n",
      "  8.45833682e-03 -9.91746783e-03  8.65194667e-03 -7.43541261e-03\n",
      " -6.95431884e-03 -1.00371875e-02 -9.77183878e-03 -1.39843579e-02\n",
      " -8.95169855e-04  1.55859394e-03  7.06948573e-04 -1.20236874e-02\n",
      " -6.61897007e-03  1.88166574e-02  1.05343834e-02 -6.10014703e-03\n",
      "  3.44370521e-04 -1.15718609e-02  1.24407224e-02 -9.20657907e-03\n",
      " -1.20141795e-02  6.46457367e-04  1.36278979e-02 -3.42401746e-03\n",
      "  3.08185816e-03 -5.37500763e-03  4.04363564e-05  5.04519325e-04\n",
      " -6.02172734e-03 -8.89450219e-03 -3.75589472e-03  7.26384111e-03\n",
      "  6.77864905e-03 -1.11331763e-02  6.98362524e-03 -1.94460293e-03\n",
      "  2.04802747e-03 -3.02919326e-03 -7.45199679e-04 -4.62731952e-03\n",
      "  8.56184866e-03  4.13002027e-03  1.20506063e-03 -2.35236879e-03\n",
      " -2.76422244e-03 -2.16465769e-03  2.29572086e-03  1.13141276e-02\n",
      " -5.15953219e-03  8.47001188e-03 -1.77578430e-03  1.31101646e-02\n",
      " -1.28907524e-03 -2.02934793e-03  1.50314253e-02  1.20415408e-02\n",
      "  9.16328281e-03  1.75248075e-03 -9.68162343e-03 -7.92369805e-03]\n"
     ]
    }
   ],
   "source": [
    "print(\"data type of all_vecs is \", type(all_vecs))\n",
    "print(len(all_vecs))\n",
    "print(all_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function/part can handle the TSNE graphing, display to user and such\"\"\"\n",
    "\n",
    "#def grapher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main part of the program here. Call all the functions? Save the model/embeddings.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

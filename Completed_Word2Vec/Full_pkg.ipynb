{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:02:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 11:02:39: built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The purpose of this notebook is to write a .py callable program which takes in a local library location and returns a list of BERT tokenized word2vec vectors\n",
    "\"\"\"\n",
    "\n",
    "#need to double check if we need these if they're already in Lousia's functions\n",
    "import re, string \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, PreTrainedTokenizer\n",
    "import csv\n",
    "import glob \n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "#louisa's functions imported from local download as .py, added lines = true to fix value error when reading json into df\n",
    "import Louisa_w2v_functions as w2v_functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a quick function that prompts the user for a local corpus folder name then reads all the jsons in it, they then can be all cleaned w/ Louisa's clean\n",
    "#function or a one I wrote\n",
    "\n",
    "#w2v_functions.feed2vec(r\"C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json\", tokenize = True )\n",
    "\n",
    "def super_list_maker(separate = True):\n",
    "    \"\"\"this function prompts the user for a local folder/corpus location, where all the json's are located. It then reads all of them into a list.\n",
    "    Currently it reads them into a list of lists, where each book is a list. Might be useful to have this distinction\"\"\"\n",
    "    initPath = input(\"Enter a file location\")\n",
    "    bookType = input(\"Enter a data type(.json please)\")\n",
    "    token_list = []\n",
    "    final_list = []\n",
    "    sep_token_list_of_lists = []\n",
    "    if bookType:\n",
    "        path = initPath + \"\\*\" + bookType\n",
    "    else:\n",
    "        path = initPath\n",
    "    raw_path = r\"{}\".format(path)\n",
    "    bookList = glob.glob(raw_path)\n",
    "    \n",
    "    yesOrNo = input(\"Is there an additional file type? (y or n) \")\n",
    "    if yesOrNo == \"y\":\n",
    "        bookType2 = input(\"Enter a second data type(.txt) \")\n",
    "        path2 = initPath + \"\\*\" + bookType2\n",
    "        raw_path2 = r\"{}\".format(path2)\n",
    "        bookList2 = glob.glob(raw_path2)\n",
    "        for bookSite in bookList2:\n",
    "            bookList.append(bookSite)\n",
    "    \n",
    "    bookCount = len(bookList)\n",
    "    print(\"bookCount is \", bookCount)\n",
    "    print(\"book list is \", bookList)\n",
    "\n",
    "    \n",
    "    for i in range(bookCount): #(len(bookList)):\n",
    "        sep_book_list = w2v_functions.feed2vec(bookList[i], tokenize = True)\n",
    "        sep_token_list_of_lists.append(sep_book_list[0])\n",
    "        print(\"token list as of book \", i+1, \"is \", sep_book_list[0])\n",
    "        print(\"tokens in book \", i+1, \"is\", len(sep_book_list[0]))\n",
    "        print()\n",
    "        for word in sep_book_list[0]:\n",
    "            token_list.append(word)  #use true here\n",
    "        print(\"data type of token_list[\", i+1, \"]\", type(token_list[i]))\n",
    "        #sep_token_list_of_lists.append(token_list[i])\n",
    "       # if type(token_list[i]) == list:\n",
    "       #     for subtoken in token_list[i]:\n",
    "       #         final_list.append(subtoken)\n",
    "\n",
    "    print()\n",
    "    print(\"length of vocab \", len(token_list))\n",
    "    \n",
    "    if separate == False:\n",
    "        print(\"final list length/token count \", len(token_list))\n",
    "        print (\"all tokens \", token_list)\n",
    "        return token_list\n",
    "    else:\n",
    "        print(\"list of token lists \", sep_token_list_of_lists)\n",
    "        return sep_token_list_of_lists\n",
    "    print()\n",
    "\n",
    "#use a default variable here to try list of lists vs singular token list?\n",
    "\n",
    "#local book path = C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files   this format is what works, then use .json as type  \n",
    "#super_list_maker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a file location C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\n",
      "Enter a data type(.json please) .json\n",
      "Is there an additional file type? (y or n)  y\n",
      "Enter a second data type(.txt)  .txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookCount is  7\n",
      "book list is  ['C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Polymer_Chemistry_by_Koltzsenburg_Maskos_and_Nuyken.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Polymer_Synthesis_Theory_and_Practice_by_Braun_Cherdron_Rehahn_Ritter_and_Voit.json', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Book Basic Principles of Organic Chemistry (Roberts and Caserio).txt', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Book Logic of Organic Synthesis (Rao).txt', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Book Organic Chemistry - A Carbonyl Early Approach (McMichael).txt', 'C:\\\\Users\\\\bowri\\\\square1\\\\ChemLP\\\\Bowman\\\\textbook_files\\\\Soderberg_bio_o_chem.txt']\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Brewing_Science_A_Multidisciplinary_Approach_by_Mosher_and_Trantham.json\n",
      "                                                   0\n",
      "0  michael mosher · kenneth trantham  brewing  sc...\n",
      "1  all rights are reserved by the publisher, whet...\n",
      "2  the use of general descriptive names, register...\n",
      "3  the publisher, the authors and the editors are...\n",
      "4  neither the publisher nor the authors or the e...\n",
      "5                     trademarks, service marks, etc\n",
      "6   printed on acid-free paper  this springer imp...\n",
      "7  thus began the discussion for the beginnings o...\n",
      "8  “wouldn’t it be awesome,” we thought, “if we c...\n",
      "9              the result is what you will find here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:04:45: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:04:45: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  1 is  ['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '4', 'neither', 'publisher', 'authors', 'editors', 'give', 'warrant', '.', '.', '.', '.', '.', '.', '131', '##7', '##6', '131', '##7', '##7', 'k', 'g', 'k', 'j', 'k', 'p', 'r', 'n', 'e', 'g', 'k', 'j', 'k', 'p', 'l', 'h', 'n', 'e', 'g', 'k', 'j', 'k', 'g', '.', '.', '.', '131', '##7', '##8', 'b', 'e', 'l', 'b', 'appendix', 'b', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '.', '.', '.', '131', '##7', '##9', 'moshe', '##r', 'k', '131', '##80', 'tran', '##tham', 'brewing', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '81', '##7', '##9', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  1 is 146\n",
      "\n",
      "data type of token_list[ 1 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Polymer_Chemistry_by_Koltzsenburg_Maskos_and_Nuyken.json\n",
      "                                                   0\n",
      "0  sebastian koltzenburg michael maskos oskar nuy...\n",
      "1  all rights are reserved by the publisher, whet...\n",
      "2  the  use  of  general  descriptive  names,  re...\n",
      "3   in  this  publication does not imply, even in...\n",
      "4  the publisher, the authors and the editors are...\n",
      "5   neither  the  publisher  nor  the  authors or...\n",
      "6   cover illustration: with kind permission by g...\n",
      "7   all computer chips used in our desktops, lapt...\n",
      "8   cutting-edge  biomedical  applications  requi...\n",
      "9  the interior of every automobile is almost ent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:04:46: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:04:47: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  2 is  ['0', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'michael', 'mask', '##os', 'oskar', 'nu', '##y', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '.', '.', '.', '4', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '.', '.', '.', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '.', '.', '.', '950', '##5', 'gut', '##ta', 'perch', '##a', 'h', 'ha', '##f', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'h', '.', '.', '.', '950', '##6', 'see', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', 'pe', '##llet', '##izing', 'cold', 'pe', '##ll', '.', '.', '.', '950', '##7', 'see', 'forming', 'processes', 'resin', 'transfer', 'mold', '##ing', 'r', '.', '.', '.', '950', '##8', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', '##er', 'transit', '##io', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '86', '##8', '##2', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  2 is 162\n",
      "\n",
      "data type of token_list[ 2 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Polymer_Synthesis_Theory_and_Practice_by_Braun_Cherdron_Rehahn_Ritter_and_Voit.json\n",
      "                                                   0\n",
      "0          polymer synthesis: theory and practice   \n",
      "1    dietrich braun (29) harald cherdron matthias...\n",
      "2  dresden germany  isbn 978-3-642-28979-8 doi 10...\n",
      "3  all rights are reserved by the publisher, whet...\n",
      "4  exempted from this legal reservation are brief...\n",
      "5  duplication of this publication or parts there...\n",
      "6  permissions for use may be obtained through ri...\n",
      "7  violations are liable to prosecution under the...\n",
      "8  the use of general descriptive names, register...\n",
      "9  in this publication does not imply, even in th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:04:48: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:04:48: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  3 is  ['0', 'polymer', 'synthesis', 'theory', 'practice', '1', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', 're', '##ha', '##hn', '.', '.', '.', '2', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'n', '.', '.', '.', '3', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '4', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '.', '.', '.', '.', '.', '.', '122', '##60', 'braun', 'et', 'al', 'polymer', 'synthesis', 'theory', 'practice', '.', '.', '.', '122', '##6', '##1', 'see', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', '##r', 'of', '##et', 'ole', '##d', '122', '##6', '##2', 'see', 'organic', 'light', 'emi', '##tting', 'di', '##ode', 'ole', '##d', 'ol', '##igo', '##mer', '.', '.', '.', '122', '##6', '##3', 'see', 'organic', 'photo', 'voltage', 'op', '##v', 'organic', 'field', '##ef', '##f', '.', '.', '.', '122', '##64', 'see', 'polymer', 'based', 'organic', 'light', 'emi', '##tting', 'di', '##ode', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '70', '##57', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  3 is 143\n",
      "\n",
      "data type of token_list[ 3 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Book Basic Principles of Organic Chemistry (Roberts and Caserio).txt\n",
      "                                                   0\n",
      "0  1: Introduction to Organic Chemistry: You now ...\n",
      "1  2: Structural Organic Chemistry: This chapter ...\n",
      "2  3: Organic Nomenclature: A chemical nomenclatu...\n",
      "3  4: Alkanes: Alkanes are the simplest organic m...\n",
      "4  5: Stereoisomerism of Organic Molecules: Posit...\n",
      "5  6: Bonding in Organic Molecules: Remembering t...\n",
      "6  7: Other Compounds than Hydrocarbons: We begin...\n",
      "7  8: Nucleophilic Substitution and Elimination R...\n",
      "8  9: Separation, Purification, & Identification ...\n",
      "9  10: Alkenes and Alkynes I - Ionic and Radical ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:04:50: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:04:50: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  4 is  ['0', 'introduction', 'organic', 'chemistry', 'starting', 'study', '.', '.', '.', '1', 'structural', 'organic', 'chemistry', 'chapter', 'briefly', 'r', '.', '.', '.', '2', 'organic', 'nomenclature', 'chemical', 'nomenclature', 'set', '.', '.', '.', '3', 'al', '##kan', '##es', 'al', '##kan', '##es', 'simplest', 'organic', 'molecules', 'con', '.', '.', '.', '4', 'stereo', '##iso', '##mer', '##ism', 'organic', 'molecules', 'position', 'iso', '.', '.', '.', '.', '.', '.', '71', '##50', 'show', 'mechanisms', 'combination', 'others', 'described', 'c', '.', '.', '.', '71', '##51', 'ch', 'ch', 'pd', '##cl', 'h', 'ch', '71', '##52', 'reaction', 'used', 'large', '##sca', '##le', 'production', 'ox', '##idi', '##zing', '.', '.', '.', '71', '##53', 'b', 'balance', 'competitive', 'nu', '##cle', '##op', '##hil', '##ic', 'reactions', 'd', '.', '.', '.', '71', '##54', 'write', 'me', '##chan', '##istic', 'steps', 'account', 'difference', 'ste', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '67', '##42', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  4 is 131\n",
      "\n",
      "data type of token_list[ 4 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Book Logic of Organic Synthesis (Rao).txt\n",
      "                                                   0\n",
      "0                  1: Synthesis of Organic Molecules\n",
      "1  2: Rules and Guidelines Governing Organic Synt...\n",
      "2          Baldwin’s Rule for Ring Closure Reactions\n",
      "3                                       Bredt's Rule\n",
      "4                      Cram's Rule and Prelog's Rule\n",
      "5                  Hofmann’s Rule and Zaitsev’s Rule\n",
      "6                                   Markovnikov Rule\n",
      "7   3: Criteria for Selection of the Synthetic Route\n",
      "8                          4: The Logic of Synthesis\n",
      "9              5: Strategies in Disparlure Synthesis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:04:51: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:04:51: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  5 is  ['0', 'synthesis', 'organic', 'molecules', '1', 'rules', 'guidelines', 'governing', 'organic', 'synthesis', '2', 'baldwin', '’', 's', 'rule', 'ring', 'closure', 'reactions', '3', 'bred', '##ts', 'rule', '4', 'cr', '##ams', 'rule', 'pre', '##log', '##s', 'rule', '.', '.', '.', '580', 'fig', '58', '##1', 'use', 'transition', 'metal', 're', '##age', '##nts', 'could', 'aid', 'con', '##st', '##r', '.', '.', '.', '58', '##2', 'fig', '58', '##3', 'rare', 'molecular', 'engineering', 'marvel', '##s', 'like', 'rb', 'woo', '.', '.', '.', '58', '##4', 'doubt', 'feasibility', 'protection', 'free', 'synthesis', 'co', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '49', '##9', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  5 is 91\n",
      "\n",
      "data type of token_list[ 5 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Book Organic Chemistry - A Carbonyl Early Approach (McMichael).txt\n",
      "                                                   0\n",
      "0    1: Carbonyl Group: Notation, Structure, Bonding\n",
      "1        2: Functional Groups, Hybridization, Naming\n",
      "2       3: Additions: Electrophilic and Nucleophilic\n",
      "3          4: Acetal Formation, Mechanism, Resonance\n",
      "4         5: Nitrogen Nucleophiles - Imine Formation\n",
      "5          6: Addition of Organometallics - Grignard\n",
      "6        7: Oxidation & Reduction, alpha-C-H acidity\n",
      "7         8: Enolates, Aldol Condensation, Synthesis\n",
      "8    9: Carboxylic Acid Derivatives: Interconversion\n",
      "9  10: Carboxylic Acid Derivatives - Alpha Carbon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:04:52: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:04:52: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  6 is  ['0', 'carbon', '##yl', 'group', 'notation', 'structure', 'bonding', '1', 'functional', 'groups', 'hybrid', '##ization', 'naming', '2', 'additions', 'electro', '##phi', '##lic', 'nu', '##cle', '##op', '##hil', '##ic', '3', 'ace', '##tal', 'formation', 'mechanism', 'resonance', '4', 'nitrogen', 'nu', '##cle', '##op', '##hil', '##es', 'im', '##ine', 'formation', '.', '.', '.', '101', '##8', 'finally', 'lets', 'return', 'another', 'application', 'free', 'r', '.', '.', '.', '102', '##0', 'new', 'molecule', 'would', 'also', 'free', 'radical', 'could', 'add', '.', '.', '.', '102', '##1', 'polymer', '##ization', 'reactions', 'also', 'important', 'table', '.', '.', '.', '102', '##3', 'since', 'repeating', 'unit', 'formed', 'adding', 'end', 'double', '.', '.', '.', '102', '##4', 'kirk', 'mc', '##mic', '##hae', '##l', 'washington', 'state', 'university', 'name', ':', 'text', ',', 'length', ':', '84', '##8', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  6 is 113\n",
      "\n",
      "data type of token_list[ 6 ] <class 'str'>\n",
      "filepath is  C:\\Users\\bowri\\square1\\ChemLP\\Bowman\\textbook_files\\Soderberg_bio_o_chem.txt\n",
      "                                                   0\n",
      "0  1: Introduction to Organic Structure and Bondi...\n",
      "1  2: Introduction to Organic Structure and Bondi...\n",
      "2               3: Conformations and Stereochemistry\n",
      "3  4: Structure Determination I- UV-Vis and Infra...\n",
      "4  5: Structure Determination Part II - Nuclear M...\n",
      "5                  6: Overview of Organic Reactivity\n",
      "6                             7: Acid-base Reactions\n",
      "7             8: Nucleophilic Substitution Reactions\n",
      "8  9: Phosphate Transfer Reactions: This chapter ...\n",
      "9       10: Nucleophilic Carbonyl Addition Reactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:04:53: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bowri/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "WARNING - 11:04:53: Keyword arguments {'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list as of book  7 is  ['0', 'introduction', 'organic', 'structure', 'bonding', 'chapter', '.', '.', '.', '1', 'introduction', 'organic', 'structure', 'bonding', 'ii', '2', 'conform', '##ations', 'stereo', '##chemist', '##ry', '3', 'structure', 'determination', 'uv', '##vis', 'infrared', 'spec', '##tro', '.', '.', '.', '4', 'structure', 'determination', 'part', 'ii', 'nuclear', 'magnet', '.', '.', '.', '.', '.', '.', '425', '##6', 'phase', '425', '##7', 'phase', '425', '##8', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', '425', '##9', 'completing', 'chapter', 'able', '42', '##60', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '40', '##8', '##1', ',', 'dt', '##ype', ':', 'object']\n",
      "tokens in book  7 is 94\n",
      "\n",
      "data type of token_list[ 7 ] <class 'str'>\n",
      "\n",
      "length of vocab  880\n",
      "list of token lists  [['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '4', 'neither', 'publisher', 'authors', 'editors', 'give', 'warrant', '.', '.', '.', '.', '.', '.', '131', '##7', '##6', '131', '##7', '##7', 'k', 'g', 'k', 'j', 'k', 'p', 'r', 'n', 'e', 'g', 'k', 'j', 'k', 'p', 'l', 'h', 'n', 'e', 'g', 'k', 'j', 'k', 'g', '.', '.', '.', '131', '##7', '##8', 'b', 'e', 'l', 'b', 'appendix', 'b', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '.', '.', '.', '131', '##7', '##9', 'moshe', '##r', 'k', '131', '##80', 'tran', '##tham', 'brewing', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '81', '##7', '##9', ',', 'dt', '##ype', ':', 'object'], ['0', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'michael', 'mask', '##os', 'oskar', 'nu', '##y', '.', '.', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '2', 'use', 'general', 'descriptive', 'names', 'registered', 'names', '.', '.', '.', '3', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '.', '.', '.', '4', 'publisher', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '.', '.', '.', '.', '.', '.', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '.', '.', '.', '950', '##5', 'gut', '##ta', 'perch', '##a', 'h', 'ha', '##f', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'h', '.', '.', '.', '950', '##6', 'see', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', 'pe', '##llet', '##izing', 'cold', 'pe', '##ll', '.', '.', '.', '950', '##7', 'see', 'forming', 'processes', 'resin', 'transfer', 'mold', '##ing', 'r', '.', '.', '.', '950', '##8', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', '##er', 'transit', '##io', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '86', '##8', '##2', ',', 'dt', '##ype', ':', 'object'], ['0', 'polymer', 'synthesis', 'theory', 'practice', '1', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', 're', '##ha', '##hn', '.', '.', '.', '2', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'n', '.', '.', '.', '3', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '.', '.', '.', '4', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '.', '.', '.', '.', '.', '.', '122', '##60', 'braun', 'et', 'al', 'polymer', 'synthesis', 'theory', 'practice', '.', '.', '.', '122', '##6', '##1', 'see', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', '##r', 'of', '##et', 'ole', '##d', '122', '##6', '##2', 'see', 'organic', 'light', 'emi', '##tting', 'di', '##ode', 'ole', '##d', 'ol', '##igo', '##mer', '.', '.', '.', '122', '##6', '##3', 'see', 'organic', 'photo', 'voltage', 'op', '##v', 'organic', 'field', '##ef', '##f', '.', '.', '.', '122', '##64', 'see', 'polymer', 'based', 'organic', 'light', 'emi', '##tting', 'di', '##ode', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '70', '##57', ',', 'dt', '##ype', ':', 'object'], ['0', 'introduction', 'organic', 'chemistry', 'starting', 'study', '.', '.', '.', '1', 'structural', 'organic', 'chemistry', 'chapter', 'briefly', 'r', '.', '.', '.', '2', 'organic', 'nomenclature', 'chemical', 'nomenclature', 'set', '.', '.', '.', '3', 'al', '##kan', '##es', 'al', '##kan', '##es', 'simplest', 'organic', 'molecules', 'con', '.', '.', '.', '4', 'stereo', '##iso', '##mer', '##ism', 'organic', 'molecules', 'position', 'iso', '.', '.', '.', '.', '.', '.', '71', '##50', 'show', 'mechanisms', 'combination', 'others', 'described', 'c', '.', '.', '.', '71', '##51', 'ch', 'ch', 'pd', '##cl', 'h', 'ch', '71', '##52', 'reaction', 'used', 'large', '##sca', '##le', 'production', 'ox', '##idi', '##zing', '.', '.', '.', '71', '##53', 'b', 'balance', 'competitive', 'nu', '##cle', '##op', '##hil', '##ic', 'reactions', 'd', '.', '.', '.', '71', '##54', 'write', 'me', '##chan', '##istic', 'steps', 'account', 'difference', 'ste', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '67', '##42', ',', 'dt', '##ype', ':', 'object'], ['0', 'synthesis', 'organic', 'molecules', '1', 'rules', 'guidelines', 'governing', 'organic', 'synthesis', '2', 'baldwin', '’', 's', 'rule', 'ring', 'closure', 'reactions', '3', 'bred', '##ts', 'rule', '4', 'cr', '##ams', 'rule', 'pre', '##log', '##s', 'rule', '.', '.', '.', '580', 'fig', '58', '##1', 'use', 'transition', 'metal', 're', '##age', '##nts', 'could', 'aid', 'con', '##st', '##r', '.', '.', '.', '58', '##2', 'fig', '58', '##3', 'rare', 'molecular', 'engineering', 'marvel', '##s', 'like', 'rb', 'woo', '.', '.', '.', '58', '##4', 'doubt', 'feasibility', 'protection', 'free', 'synthesis', 'co', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '49', '##9', ',', 'dt', '##ype', ':', 'object'], ['0', 'carbon', '##yl', 'group', 'notation', 'structure', 'bonding', '1', 'functional', 'groups', 'hybrid', '##ization', 'naming', '2', 'additions', 'electro', '##phi', '##lic', 'nu', '##cle', '##op', '##hil', '##ic', '3', 'ace', '##tal', 'formation', 'mechanism', 'resonance', '4', 'nitrogen', 'nu', '##cle', '##op', '##hil', '##es', 'im', '##ine', 'formation', '.', '.', '.', '101', '##8', 'finally', 'lets', 'return', 'another', 'application', 'free', 'r', '.', '.', '.', '102', '##0', 'new', 'molecule', 'would', 'also', 'free', 'radical', 'could', 'add', '.', '.', '.', '102', '##1', 'polymer', '##ization', 'reactions', 'also', 'important', 'table', '.', '.', '.', '102', '##3', 'since', 'repeating', 'unit', 'formed', 'adding', 'end', 'double', '.', '.', '.', '102', '##4', 'kirk', 'mc', '##mic', '##hae', '##l', 'washington', 'state', 'university', 'name', ':', 'text', ',', 'length', ':', '84', '##8', ',', 'dt', '##ype', ':', 'object'], ['0', 'introduction', 'organic', 'structure', 'bonding', 'chapter', '.', '.', '.', '1', 'introduction', 'organic', 'structure', 'bonding', 'ii', '2', 'conform', '##ations', 'stereo', '##chemist', '##ry', '3', 'structure', 'determination', 'uv', '##vis', 'infrared', 'spec', '##tro', '.', '.', '.', '4', 'structure', 'determination', 'part', 'ii', 'nuclear', 'magnet', '.', '.', '.', '.', '.', '.', '425', '##6', 'phase', '425', '##7', 'phase', '425', '##8', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', '425', '##9', 'completing', 'chapter', 'able', '42', '##60', 'organic', 'chemistry', 'biological', 'emphasis', 'tim', 'so', '##de', '.', '.', '.', 'name', ':', 'text', ',', 'length', ':', '40', '##8', '##1', ',', 'dt', '##ype', ':', 'object']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:04:53: collecting all words and their counts\n",
      "INFO - 11:04:53: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 11:04:53: collected 382 word types from a corpus of 880 raw words and 7 sentences\n",
      "INFO - 11:04:53: Loading a fresh vocabulary\n",
      "INFO - 11:04:53: effective_min_count=1 retains 382 unique words (100% of original 382, drops 0)\n",
      "INFO - 11:04:53: effective_min_count=1 leaves 880 word corpus (100% of original 880, drops 0)\n",
      "INFO - 11:04:53: deleting the raw counts dictionary of 382 items\n",
      "INFO - 11:04:53: sample=0.001 downsamples 74 most-common words\n",
      "INFO - 11:04:53: downsampling leaves estimated 584 word corpus (66.5% of prior 880)\n",
      "INFO - 11:04:53: estimated required memory for 382 words and 500 dimensions: 1719000 bytes\n",
      "INFO - 11:04:53: resetting layer weights\n",
      "INFO - 11:04:53: training model with 3 workers on 382 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 1 : training on 880 raw words (580 effective words) took 0.0s, 82037 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 2 : training on 880 raw words (580 effective words) took 0.0s, 87036 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 3 : training on 880 raw words (587 effective words) took 0.0s, 100179 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 4 : training on 880 raw words (576 effective words) took 0.0s, 77895 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 5 : training on 880 raw words (598 effective words) took 0.0s, 82124 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 6 : training on 880 raw words (594 effective words) took 0.0s, 87415 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 7 : training on 880 raw words (592 effective words) took 0.0s, 96480 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 8 : training on 880 raw words (584 effective words) took 0.0s, 87531 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 9 : training on 880 raw words (586 effective words) took 0.0s, 81235 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 10 : training on 880 raw words (597 effective words) took 0.0s, 80705 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 11 : training on 880 raw words (574 effective words) took 0.0s, 93716 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 12 : training on 880 raw words (575 effective words) took 0.0s, 87550 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 13 : training on 880 raw words (583 effective words) took 0.0s, 76940 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 14 : training on 880 raw words (587 effective words) took 0.0s, 85108 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 15 : training on 880 raw words (590 effective words) took 0.0s, 78961 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 16 : training on 880 raw words (593 effective words) took 0.0s, 85413 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 17 : training on 880 raw words (590 effective words) took 0.0s, 88395 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 18 : training on 880 raw words (577 effective words) took 0.0s, 65006 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 19 : training on 880 raw words (585 effective words) took 0.0s, 70502 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 20 : training on 880 raw words (588 effective words) took 0.0s, 56740 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 21 : training on 880 raw words (574 effective words) took 0.0s, 62931 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 22 : training on 880 raw words (577 effective words) took 0.0s, 74258 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 23 : training on 880 raw words (585 effective words) took 0.0s, 84081 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 24 : training on 880 raw words (578 effective words) took 0.0s, 79279 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:53: EPOCH - 25 : training on 880 raw words (590 effective words) took 0.0s, 97453 effective words/s\n",
      "INFO - 11:04:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:54: EPOCH - 26 : training on 880 raw words (587 effective words) took 0.0s, 97899 effective words/s\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:54: EPOCH - 27 : training on 880 raw words (583 effective words) took 0.0s, 93489 effective words/s\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:54: EPOCH - 28 : training on 880 raw words (598 effective words) took 0.0s, 97429 effective words/s\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:54: EPOCH - 29 : training on 880 raw words (600 effective words) took 0.0s, 85849 effective words/s\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:04:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:04:54: EPOCH - 30 : training on 880 raw words (592 effective words) took 0.0s, 86380 effective words/s\n",
      "INFO - 11:04:54: training on a 26400 raw words (17580 effective words) took 0.4s, 47834 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made it here!\n",
      "Time to train the model: 0.01 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:04:54: saving Word2VecKeyedVectors object under C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv, separately None\n",
      "INFO - 11:04:54: not storing attribute vectors_norm\n",
      "INFO - 11:04:54: saved C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv\n",
      "INFO - 11:04:54: loading Word2VecKeyedVectors object from C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv\n",
      "INFO - 11:04:54: setting ignored attribute vectors_norm to None\n",
      "INFO - 11:04:54: loaded C:\\Users\\bowri\\AppData\\Local\\Temp\\vectors.kv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus count is  7\n",
      "epochs is  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:04:54: saving Word2Vec object under fourBook, separately None\n",
      "INFO - 11:04:54: not storing attribute vectors_norm\n",
      "INFO - 11:04:54: not storing attribute cum_table\n",
      "INFO - 11:04:54: saved fourBook\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyed vectors  <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000027A05652308>\n",
      "all_vectors(done via vocab)  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_vectors type is  <class 'list'>\n",
      "all words in wv.vocab are  ['0', 'michael', 'moshe', '##r', '·', 'kenneth', 'tran', '##tham', 'brewing', 'sci', '##e', '.', '1', 'rights', 'reserved', 'publisher', 'whether', 'whole', 'part', 'm', '2', 'use', 'general', 'descriptive', 'names', 'registered', '3', 'authors', 'editors', 'safe', 'assume', 'advice', 'i', '4', 'neither', 'give', 'warrant', '131', '##7', '##6', 'k', 'g', 'j', 'p', 'r', 'n', 'e', 'l', 'h', '##8', 'b', 'appendix', 'ref', '##ri', '##ger', '##ant', 'data', 'table', 'su', '##pe', '##9', '##80', 'science', 'multi', '##dis', '##ci', '##plin', '##ary', 'app', 'name', ':', 'text', ',', 'length', '81', 'dt', '##ype', 'object', 'sebastian', 'ko', '##lt', '##zen', '##burg', 'mask', '##os', 'oskar', 'nu', '##y', 'publication', 'imply', 'even', 'absence', 'specific', 'state', '##m', '950', '##4', 'see', 'blow', 'mold', '##ing', 'f', 'fenton', '’', 's', 're', '##age', '##nt', 'fiber', '##re', '##in', '##f', '##5', 'gut', '##ta', 'perch', '##a', 'ha', '##nium', 'half', '##san', '##d', '##wich', 'compound', 'poly', '##eth', '##er', 'ke', '##tone', 'pe', '##k', '##llet', '##izing', 'cold', '##ll', 'forming', 'processes', 'resin', 'transfer', 'sc', '##hul', '##z', '##fl', '##ory', 'distribution', 'second', '##ord', 'transit', '##io', '86', '##2', 'polymer', 'synthesis', 'theory', 'practice', 'dietrich', 'braun', 'harald', 'cher', '##dron', 'matthias', '##ha', '##hn', 'dresden', 'germany', 'isbn', 'doi', 'springer', 'heidelberg', 'exempt', '##ed', 'legal', 'reservation', 'brief', 'excerpts', 'con', '##n', '122', '##60', 'et', 'al', '##1', 'organic', 'field', '##ef', '##fect', 'trans', '##isto', 'of', '##et', 'ole', 'light', 'emi', '##tting', 'di', '##ode', 'ol', '##igo', '##mer', '##3', 'photo', 'voltage', 'op', '##v', '##64', 'based', '70', '##57', 'introduction', 'chemistry', 'starting', 'study', 'structural', 'chapter', 'briefly', 'nomenclature', 'chemical', 'set', '##kan', '##es', 'simplest', 'molecules', 'stereo', '##iso', '##ism', 'position', 'iso', '71', '##50', 'show', 'mechanisms', 'combination', 'others', 'described', 'c', '##51', 'ch', 'pd', '##cl', '##52', 'reaction', 'used', 'large', '##sca', '##le', 'production', 'ox', '##idi', '##zing', '##53', 'balance', 'competitive', '##cle', '##op', '##hil', '##ic', 'reactions', 'd', '##54', 'write', 'me', '##chan', '##istic', 'steps', 'account', 'difference', 'ste', '67', '##42', 'rules', 'guidelines', 'governing', 'baldwin', 'rule', 'ring', 'closure', 'bred', '##ts', 'cr', '##ams', 'pre', '##log', '##s', '580', 'fig', '58', 'transition', 'metal', '##nts', 'could', 'aid', '##st', 'rare', 'molecular', 'engineering', 'marvel', 'like', 'rb', 'woo', 'doubt', 'feasibility', 'protection', 'free', 'co', '49', 'carbon', '##yl', 'group', 'notation', 'structure', 'bonding', 'functional', 'groups', 'hybrid', '##ization', 'naming', 'additions', 'electro', '##phi', '##lic', 'ace', '##tal', 'formation', 'mechanism', 'resonance', 'nitrogen', 'im', '##ine', '101', 'finally', 'lets', 'return', 'another', 'application', '102', '##0', 'new', 'molecule', 'would', 'also', 'radical', 'add', 'important', 'since', 'repeating', 'unit', 'formed', 'adding', 'end', 'double', 'kirk', 'mc', '##mic', '##hae', '##l', 'washington', 'university', '84', 'ii', 'conform', '##ations', '##chemist', '##ry', 'determination', 'uv', '##vis', 'infrared', 'spec', '##tro', 'nuclear', 'magnet', '425', 'phase', 'biological', 'emphasis', 'tim', 'so', '##de', 'completing', 'able', '42', '40']\n",
      "number of words in wv.vocab is  382\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This function needs to take in a list of tokens, set them as the global tokens value for Louisa's functions then train the model with them and return\n",
    "the word embeddings\n",
    "\"\"\"\n",
    "\n",
    "#take in a vocab list\n",
    "\n",
    "list_of_tokens = super_list_maker()\n",
    "#saveName = input(\"Enter a name for the model saved: \")\n",
    "w2v_functions.tokens = list_of_tokens\n",
    "all_vecs = w2v_functions.w2v_train(\"fourBook\", min_count = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type of all_vecs is  <class 'list'>\n",
      "382\n",
      "[ 5.00144670e-03 -5.85159753e-03  8.28532211e-04 -6.21038908e-03\n",
      "  1.56986073e-03 -7.74002913e-03 -7.17886491e-03  6.52007898e-03\n",
      " -2.09283363e-02 -1.09173395e-02  2.25867759e-02  3.27195297e-03\n",
      " -6.99316384e-03 -1.62399318e-02 -1.48725128e-02 -1.24684153e-02\n",
      "  9.29844938e-03 -9.00108460e-03 -3.93491052e-03  2.91640777e-03\n",
      " -2.15287302e-02 -1.54902483e-03  4.07102145e-03  1.18109845e-02\n",
      "  1.97994011e-03 -8.05578101e-03  1.82125457e-02 -5.24422526e-03\n",
      "  4.44077794e-03  7.52322190e-03  2.97036924e-04  2.30542547e-03\n",
      " -4.89422600e-05 -1.10560562e-02 -9.94485337e-03  1.47800995e-02\n",
      "  1.18488083e-02 -6.06539985e-03 -1.22764912e-02 -1.79076754e-02\n",
      "  4.77751996e-03  1.21954028e-02 -1.13627398e-02  8.09182297e-04\n",
      " -3.68858106e-03 -1.22039123e-02 -4.98529896e-03 -5.24759898e-03\n",
      "  8.82213749e-03 -1.04142567e-02  2.04482093e-03 -2.41747759e-02\n",
      "  1.27233127e-02  1.94084912e-03  2.10079588e-02 -2.06120545e-03\n",
      "  1.70673162e-03  6.45194901e-04 -9.75085795e-03 -4.40833578e-03\n",
      "  3.08364187e-03  1.37044303e-02 -1.28396566e-03 -2.19087373e-03\n",
      "  8.17354710e-04 -1.16016278e-02 -1.23804295e-03  1.31650548e-02\n",
      "  3.98259889e-03 -5.70809282e-03 -5.13944251e-04 -1.20564783e-02\n",
      "  6.76155183e-03  2.56958301e-03  3.96654150e-03  8.75207875e-03\n",
      " -3.42697580e-03 -1.40543957e-03  1.71686232e-03  8.25924706e-03\n",
      "  3.48743379e-05  1.12012168e-02 -2.02202313e-02  2.39288644e-03\n",
      " -4.17553540e-03  6.37170975e-04  1.02806790e-02 -6.20487239e-03\n",
      "  5.30269499e-05 -7.67106609e-03  8.94793216e-03  3.10830935e-03\n",
      "  9.88886598e-03 -2.68777995e-03 -1.72559805e-02  8.81301748e-05\n",
      " -5.97480731e-03 -7.59353023e-03 -1.78428169e-03 -2.00485159e-02\n",
      " -3.05709452e-03  5.46400528e-03 -4.53380076e-03 -6.86370186e-04\n",
      "  1.29610579e-02  1.05645340e-02 -9.49755032e-03  2.11841781e-02\n",
      " -2.75775744e-03 -1.62962195e-03  5.12058474e-03  4.61916585e-04\n",
      " -4.15319065e-03 -9.24233533e-03 -1.49827509e-03 -2.48729880e-03\n",
      " -5.38641913e-03  6.40384434e-03  1.28604239e-02 -1.73406433e-02\n",
      " -6.42178440e-03  1.14240227e-02  8.31583142e-03  6.87410415e-04\n",
      "  1.06310062e-02  3.06163705e-03 -4.77983663e-03 -3.23208980e-03\n",
      "  3.75617621e-03  1.02513172e-02 -1.45310936e-02  1.99766625e-02\n",
      "  1.96384508e-02  5.70790842e-03 -1.18253389e-04  1.35678693e-03\n",
      " -5.54855773e-03 -5.92494244e-03  1.74109638e-03 -1.87978055e-02\n",
      "  1.30946079e-04 -9.87566169e-03  9.85327456e-03  3.74838593e-03\n",
      " -6.15405710e-03 -3.77470208e-03  1.31122451e-02 -8.30317847e-03\n",
      " -7.68755344e-05  7.99568743e-03  5.32331364e-03 -1.43008446e-03\n",
      "  2.29923730e-03  7.21453049e-04 -8.28130078e-03  6.55211136e-03\n",
      "  9.26352479e-03 -1.26730269e-02 -8.68793391e-03 -2.18154141e-03\n",
      "  1.80994160e-02  1.40805414e-03  1.11426935e-02  7.45229889e-03\n",
      "  1.03265010e-02  1.22870980e-02 -3.21251038e-03  7.40874605e-03\n",
      "  5.30398509e-04  3.99763510e-03 -3.17750149e-03 -1.53085019e-03\n",
      "  5.49721904e-03  1.25248916e-03 -5.09619201e-03  3.31512582e-03\n",
      " -8.11394863e-03  9.44364164e-03 -1.51582379e-02 -8.31239298e-03\n",
      "  1.83776268e-04  9.59079526e-03 -1.90100428e-02 -4.87973820e-03\n",
      "  1.08458363e-02 -1.33218206e-02 -1.29196197e-02 -9.55958851e-03\n",
      " -8.46761931e-03 -5.20912046e-03 -1.57398619e-02  5.56555670e-03\n",
      "  9.90649313e-03 -4.47453046e-03 -3.13457008e-03  6.41462998e-03\n",
      " -3.09820706e-03 -6.75321370e-03 -1.72643096e-03 -8.63656309e-03\n",
      "  8.94725975e-03  4.12181905e-03 -1.74290556e-02 -4.00491524e-03\n",
      " -9.97055788e-03 -7.26074504e-04  7.63122644e-03  1.05477236e-02\n",
      "  9.09511279e-03  5.41021582e-03 -8.37156735e-03 -1.03915753e-02\n",
      "  5.59107587e-03  6.27743546e-03  7.58638466e-03 -5.29372506e-03\n",
      "  3.46291857e-03  2.22992543e-02  4.25492413e-03  2.33795289e-02\n",
      " -6.98148133e-03  1.33443354e-02 -2.48628855e-02  7.15414761e-03\n",
      "  7.28499191e-03  1.45759182e-02  1.76189223e-03  7.00268568e-03\n",
      "  3.16873635e-03 -1.10838860e-02 -2.05030181e-02  2.93221814e-03\n",
      "  7.28659471e-03 -1.23438751e-02  2.50776298e-03  6.36661611e-03\n",
      "  6.16134144e-03 -5.93011919e-03 -5.43838274e-03  4.85749915e-03\n",
      "  6.85465615e-03 -7.46917818e-03  1.31371757e-02 -2.15969281e-03\n",
      " -1.07533820e-02  5.72814513e-03 -4.20165388e-03  1.52789224e-02\n",
      "  6.22541271e-03  6.69115176e-03  7.86208198e-04 -7.82180391e-03\n",
      " -6.03081193e-03 -2.47349800e-03  7.94330146e-03 -4.87127621e-03\n",
      "  6.99282950e-03 -9.33713978e-04 -8.92350823e-03  1.73810534e-02\n",
      " -4.12153732e-03  8.67592520e-04 -6.35895645e-04  2.43067485e-03\n",
      " -9.59529076e-03 -4.64203907e-03  4.79913596e-03  1.59523450e-02\n",
      "  4.12971107e-03  5.24645089e-04  1.62229687e-03 -7.20685115e-03\n",
      "  4.52080509e-03 -3.59892892e-03  7.26362783e-03 -1.32884476e-02\n",
      " -1.67491511e-02 -1.45970229e-02  4.33254027e-05  2.50500045e-03\n",
      " -1.76763453e-03 -8.34491162e-04 -1.26576666e-02 -2.41670129e-03\n",
      " -1.30913900e-02 -1.63503997e-02 -4.50351136e-03  9.97252017e-03\n",
      "  9.77769610e-04 -1.17535777e-02 -4.53864504e-03  6.87118387e-03\n",
      " -6.84727961e-03  5.71898883e-03 -3.67354578e-03  9.50166956e-03\n",
      "  9.81389638e-03 -3.39155842e-04  6.21525710e-03  5.49916644e-03\n",
      " -3.12048406e-03 -7.45459169e-04 -1.00500258e-02 -2.60278978e-03\n",
      "  6.99200714e-03 -2.24521793e-02  3.60228075e-03  1.63528286e-02\n",
      "  6.34514447e-03  6.91782124e-03  2.33019330e-02 -2.74052349e-04\n",
      "  5.89159597e-03 -1.23103708e-03 -8.29323381e-03 -7.63506861e-03\n",
      "  7.95225520e-03  1.36483377e-02  1.98154598e-02  2.15885276e-03\n",
      "  2.37798481e-03 -6.13280060e-03  4.53036372e-03 -2.06055138e-02\n",
      " -2.20236019e-03  1.49035892e-02  1.24932518e-02  1.39295459e-02\n",
      "  8.07633903e-03 -5.99360513e-03 -2.12993543e-03  1.20539749e-02\n",
      " -5.70925418e-03  8.95217899e-03 -4.36053466e-04 -2.03832388e-02\n",
      " -1.96315460e-02  7.52054760e-03 -9.50209796e-03 -1.11690490e-02\n",
      "  1.90927163e-02 -1.65989855e-03 -1.69181246e-02 -1.29614742e-02\n",
      " -1.42386009e-03  1.02676486e-03  1.53480633e-03  3.06886504e-04\n",
      " -8.39782413e-03  2.16447213e-03 -5.31884609e-03  1.26886461e-02\n",
      "  3.56309419e-03  4.99017211e-03 -8.34462699e-03  1.04573406e-02\n",
      "  8.33312143e-03 -6.56403787e-03 -2.77747097e-03 -9.43819527e-03\n",
      "  2.16298504e-03  8.94938130e-04 -8.02503608e-04  2.70214211e-03\n",
      " -2.17479654e-02 -2.16162461e-03  9.01410636e-03  4.12617996e-03\n",
      " -3.55094462e-03  1.55760031e-02 -4.36599570e-04 -1.17226485e-02\n",
      "  3.12603451e-03 -4.06714156e-04  6.60305703e-03  7.36359507e-03\n",
      " -2.24096072e-03 -2.40637921e-03  7.58460083e-05 -2.40304391e-03\n",
      " -4.23801271e-03 -6.49828417e-03 -4.19904152e-03 -5.31199155e-03\n",
      "  8.66270438e-03 -3.03520798e-03  1.22620519e-02  4.81717201e-04\n",
      " -3.23539763e-03 -3.68425949e-03 -6.06913213e-03 -1.46218548e-02\n",
      "  5.09550655e-03  4.19828901e-03 -8.19457986e-04  8.60108994e-03\n",
      " -5.65554110e-05  6.07006135e-04  1.32468268e-02 -9.46564972e-03\n",
      "  1.33797049e-03  3.26473336e-03  1.75311007e-02  5.12121292e-03\n",
      "  2.08768174e-02 -1.49117922e-02 -2.57611251e-03 -1.11332815e-02\n",
      " -1.26572503e-02 -6.19595125e-03  1.03891268e-02  5.29095856e-03\n",
      "  1.27954083e-03 -3.31809092e-03 -8.87778774e-03  7.36738509e-03\n",
      " -4.26254421e-03 -6.36749901e-04  7.29708420e-03  2.97407550e-03\n",
      " -1.02863451e-02  8.88831541e-03  4.31115925e-03 -2.39833212e-03\n",
      " -7.99732003e-03  1.02158391e-03  1.04408115e-02  3.64075624e-03\n",
      "  6.00695284e-03 -7.17475964e-03 -8.76073539e-03 -1.97129827e-02\n",
      " -9.14409902e-05 -3.25307017e-04 -3.71155620e-04 -8.17665923e-03\n",
      "  2.67101335e-03 -1.78388711e-02  9.32534412e-03  3.40155652e-03\n",
      " -2.64148507e-03  1.94582436e-03 -1.19757708e-02 -5.31599298e-03\n",
      " -1.96562521e-03 -7.13801058e-03 -7.64099788e-03 -2.54130899e-03\n",
      " -4.08898760e-03  1.31663596e-02 -7.72156566e-03 -8.82266555e-03\n",
      "  7.69673148e-03 -9.41653177e-03 -1.32198185e-02 -9.08160582e-03\n",
      " -1.30966393e-04  1.06770452e-02  9.47106350e-03 -7.31833512e-03\n",
      "  9.89778247e-03  9.18425154e-03 -4.95690107e-03  6.75555505e-03\n",
      "  8.31753388e-03  1.59048345e-02 -1.35504780e-02 -1.54133542e-02\n",
      " -6.73623197e-03 -2.52386648e-03 -4.90723178e-03  1.09088086e-02\n",
      "  5.90245239e-03 -2.98723206e-03  5.89753676e-04 -1.70928314e-02\n",
      "  5.85421454e-04  2.67394632e-03 -3.47357988e-03 -1.02977140e-03\n",
      "  4.23951028e-03 -7.95766711e-03  5.58836153e-03  1.22341430e-02\n",
      " -8.99110548e-03  1.08544491e-02  4.71276173e-04  9.23229009e-03\n",
      " -8.44080467e-03  1.47947948e-02 -1.07117444e-02 -3.58537212e-03\n",
      " -1.04013029e-02  1.86640699e-03 -8.85192363e-04  4.46985476e-03\n",
      "  1.00265024e-02 -4.30952897e-03  2.54371669e-03  1.06383041e-02]\n"
     ]
    }
   ],
   "source": [
    "print(\"data type of all_vecs is \", type(all_vecs))\n",
    "print(len(all_vecs))\n",
    "print(all_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This function/part can handle the TSNE graphing, display to user and such'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This function/part can handle the TSNE graphing, display to user and such\"\"\"\n",
    "\n",
    "#def grapher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMain part of the program here. Call all the functions? Save the model/embeddings.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main part of the program here. Call all the functions? Save the model/embeddings.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
